{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b716303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \n",
    "\"\"\"\n",
    "Autor='Diego Paredes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443f8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53969cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH DEL PROYECTO \n",
    "\"\"\"\n",
    "with open('../../path_base.txt') as f:\n",
    "    path_base = f.read()\n",
    "path_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables generales\n",
    "\"\"\"\n",
    "path_imagenes = 'F:/GOES/'     \n",
    "\n",
    "products = ['C13','C07','C08']\n",
    "times   = ['10','20','30','40','50','00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#Limitamos el GPU, en caso se necesite\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1715a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo2D(p,run):    \n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    mxPool_1 = tf.keras.layers.MaxPooling2D()(conv2d_1)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(mxPool_1)\n",
    "    \n",
    "    conv2d_2 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling2D()(conv2d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)\n",
    "    \n",
    "    #conv2d_3 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    #mxPool_3 = tf.keras.layers.MaxPooling2D()(conv2d_3)\n",
    "    #dropout_3  = tf.keras.layers.Dropout(0.2)(mxPool_3)\n",
    "    \n",
    "    #conv2d_4 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_3)\n",
    "    #mxPool_4 = tf.keras.layers.MaxPooling2D()(conv2d_4)\n",
    "    #dropout_4  = tf.keras.layers.Dropout(0.2)(mxPool_4)\n",
    "    \n",
    "    conv2d_5 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv2d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2589bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo3D(p,run):        \n",
    "    # Imagen                                 (6, 30, 30, 3)\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    \n",
    "    conv3d_1 = tf.keras.layers.Conv3D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(conv3d_1)\n",
    "    \n",
    "    conv3d_2 = tf.keras.layers.Conv3D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling3D()(conv3d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)    \n",
    "\n",
    "    \n",
    "    conv3d_5 = tf.keras.layers.Conv3D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv3d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(modelType, lr, paciencia):\n",
    "    \n",
    "    if modelType == 'Clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr) \n",
    "        \n",
    "        #BinaryCrossentropy() #CategoricalCrossentropy()      \n",
    "        loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif modelType == 'Regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(params,run):\n",
    "    \n",
    "    if params['meanMatrizImagen']:\n",
    "        print(f\"Creando modelo 2D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo2D(params,run)\n",
    "    else:\n",
    "        print(f\"Creando modelo 3D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo3D(params,run)\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(img, DA):\n",
    "    if DA == 1:\n",
    "        return tf.image.flip_left_right(img)\n",
    "    elif DA == 2:\n",
    "        return tf.image.flip_up_down(img)\n",
    "    elif DA == 3:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        return tf.image.flip_up_down(img)   \n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value, p,run, path_base, products, times, DA=0):\n",
    "    # imagenData[0] = XO     # imagenData[1] = XA     # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    size = int(p['margen'][run] / 2)\n",
    "\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "        \n",
    "        if DA:\n",
    "            img_decoded = applyDA(img_decoded, item['DA'])\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if p['meanMatrizImagen']:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs']) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs']:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    if p['redTipo']=='Regresion':\n",
    "        print('es regresion')\n",
    "        return tuple(itemL), float(value)\n",
    "    else:\n",
    "        return tuple(itemL), int(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p, run, dataset, path_imagenes, products, times,val_split= 0.2):\n",
    "            \n",
    "    if p['dsVal']:\n",
    "        test = pd.read_csv(p['dsVal'])\n",
    "        if p['dataset']:\n",
    "            train =  dataset.sample(frac=p['dataset'])\n",
    "            test = test.sample(frac=p['dataset'])\n",
    "        else:\n",
    "            train =  dataset\n",
    "            \n",
    "    else:\n",
    "        # Escojemos una fraccion del dataset si se ha indicado           \n",
    "        if p['dataset']:\n",
    "            train, test = train_test_split(dataset.sample(frac=p['dataset']), test_size=val_split, shuffle=True)\n",
    "        else:\n",
    "            train, test = train_test_split(dataset, test_size=val_split, shuffle=True)\n",
    "               \n",
    "    inputsList = {}\n",
    "    inputsListTest = {}\n",
    "    # Agregamos un atributo para indicar que el dato va realizar DA\n",
    "    if p['DA']:        \n",
    "        inputsList['DA'] = train['DA'].tolist() \n",
    "        #inputsListTest['DA'] = test['DA'].tolist() \n",
    "        \n",
    "        \n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}')    \n",
    "    \n",
    "    for inp in p['inputs']:\n",
    "        inputsList[inp] = train[inp].tolist()  \n",
    "        inputsListTest[inp] = test[inp].tolist()  \n",
    "        \n",
    "       \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsListTest),test[p['outputs']].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,p['DA']))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,False))#p['DA']\n",
    "       \n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])#.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(p['batch'])#.prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5adbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDir(path, newDir):\n",
    "    try:\n",
    "        pathT = os.path.join(path, newDir)\n",
    "        os.mkdir(pathT)\n",
    "        return pathT\n",
    "    except FileExistsError:\n",
    "        return pathT\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear el directorio: {newDir}\")\n",
    "        pritn(f'Path base: {path}')\n",
    "        pritn(f'Nuevo    : {newDir}')        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c195161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializarVariables(repDir, params,ds):\n",
    "    # Leemos dataset\n",
    "    try:\n",
    "        dataset = pd.read_csv(ds)        \n",
    "        dsName = ds.split('/')[-1][:-4]\n",
    "    except:\n",
    "        print(f'No se pudo leer el dataset {ds}')\n",
    "        return None\n",
    "    \n",
    "    statsDir = crearDir(repDir, dsName)\n",
    "    excelFile = f'{statsDir}/Stats_{params[\"redTipo\"]}_{params[\"outputs\"]}_{datetime.today().strftime(\"%Y%m\")}.xlsx'\n",
    "    \n",
    "    if params['record'] and not os.path.exists(excelFile):\n",
    "        writer = pd.ExcelWriter(excelFile, engine = 'xlsxwriter')\n",
    "        keys_values = params.items()\n",
    "        strParams = {str(key): str(value) for key, value in keys_values}\n",
    "        strParams['dsName'] = dsName\n",
    "        pd.DataFrame(strParams,index=[0]).to_excel(writer, sheet_name = 'Informacion')          \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "    return statsDir, excelFile, dataset            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03005a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCM(logs):\n",
    "    lKeys = list(logs.keys())\n",
    "    \n",
    "    try:\n",
    "        TN = int(logs[[x for x in lKeys if 'val_true_negatives' in x][0]])\n",
    "        TP = int(logs[[x for x in lKeys if 'val_true_positives' in x][0]])\n",
    "        FN = int(logs[[x for x in lKeys if 'val_false_negatives' in x][0]])\n",
    "        FP = int(logs[[x for x in lKeys if 'val_false_positives' in x][0]])\n",
    "    except:\n",
    "        print(f'\\nNo se pudo leer keys para la matriz de confucion en logs : {lKeys}')\n",
    "        print(f'Se intento leer: val_true_negatives,val_true_positives, val_false_negatives y val_false_positives')\n",
    "    \n",
    "       \n",
    "    y_true =  [0]*TN + [1]*TP + [1]*FN + [0]*FP\n",
    "    _y_pred = [0]*TN + [1]*TP + [0]*FN + [1]*FP\n",
    "    \n",
    "    return TN, FP, FN, TP, np.array(y_true), np.array(_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d2f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCB(Callback):\n",
    "    \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "    def __init__(self, val_ds=None, WANDB=True):     \n",
    "        self.val_ds = val_ds  \n",
    "        self.history = {}\n",
    "        self.wandb = WANDB\n",
    "   \n",
    "   \n",
    "    def on_epoch_end(self, epoch, logs={}):  \n",
    "        TN, FP, FN, TP, y_true, _y_pred = getCM(logs)\n",
    "        \n",
    "        self.history.setdefault('loss', []).append(logs['loss']) \n",
    "        self.history.setdefault('acc', []).append(logs['acc'])  \n",
    "        self.history.setdefault('val_loss', []).append(logs['val_loss']) \n",
    "        self.history.setdefault('val_acc', []).append(logs['val_acc']) \n",
    "        \n",
    "        \n",
    "        self.history.setdefault('val_TN', []).append(TN) \n",
    "        self.history.setdefault('val_FP', []).append(FP)\n",
    "        self.history.setdefault('val_FN', []).append(FN) \n",
    "        self.history.setdefault('val_TP', []).append(TP) \n",
    "        \n",
    "            \n",
    "        if self.wandb:\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                    preds=_y_pred, y_true=y_true,\n",
    "                                    class_names=[0,1]),                   \n",
    "                       'val_TN' :TN,'val_FN' :FN,'val_TP' :TP,'val_FP' :FP,\n",
    "                       'val_acc': logs['val_acc'],'loss' : logs['loss'],\n",
    "                       'val_loss': logs['val_loss'],'acc' : logs['acc']                  \n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearCallbacks(statsDir,dsName, params,run, metricas):\n",
    "    CB = metricas['callbacks']\n",
    "    \n",
    "    idModel = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = statsDir + '/Model_{epoch:02d}_' + f'{params[\"redTipo\"]}_{params[\"outputs\"]}_{idModel}.hdf5' \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                     \n",
    "                                                     verbose=1)\n",
    "    \n",
    "    # Iniciamos WANDB\n",
    "    if params['record']:        \n",
    "        CB.append(cp_callback)   \n",
    "     \n",
    "    if params['WANDB']:\n",
    "        config = dict(learning_rate=params['lr'], epochs = params['epocas'],\n",
    "             batch_size =params['batch'],architecture=\"CNN\", \n",
    "             num_classes = params['num_class'],)\n",
    "        wandb.init(project=f'{params[\"Proyect\"]}-({params[\"redTipo\"]}-{params[\"outputs\"]}-{len(params[\"inputs\"])})',            \n",
    "                   config=config,\n",
    "                   name= f'Ex_{dsName}_({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})_{idModel}')   \n",
    "                                               \n",
    "    return CB, idModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(params,path_imagenes, path_base ,products, times):   \n",
    "    idProject = datetime.today().strftime(\"%Y%m%d_%H\")\n",
    "    params[\"Proyect\"] = f'{params[\"Proyect\"]}-{idProject}'\n",
    "        \n",
    "    if '.csv' in params[\"dsDir\"]:\n",
    "        ds_files = [params[\"dsDir\"]]\n",
    "    else:\n",
    "        ds_files = [f'{params[\"dsDir\"]}{e}' for e in os.listdir(params[\"dsDir\"])]\n",
    "        ds_files = [e for e in ds_files if '.csv' in e]\n",
    "        \n",
    "    maxDS = len(ds_files) if params['maxDS']==-1 else params['maxDS']\n",
    "    print(f'Cantidad de datasets encontrados : {len(ds_files)}')    \n",
    "    print(f'Datasets a usar : {maxDS}')    \n",
    "        \n",
    "           \n",
    "    \n",
    "    all_result = []      \n",
    "    \n",
    "    repDir = crearDir(f'{path_base}/Archivos/Resultados', params[\"redTipo\"])\n",
    "    repDir = crearDir(repDir, params[\"Proyect\"])\n",
    "    \n",
    "    print(f'DIRECTORIO BASE : {repDir}')\n",
    "    \n",
    "    # Una iteracion por cada dataset existente\n",
    "    ds_i = 0\n",
    "    \n",
    "    \n",
    "    for ds in ds_files[ds_i:maxDS+1]:\n",
    "        ds_i += 1       \n",
    "        \n",
    "        # Crear carpeta para DS y archivo de stats y el ds\n",
    "        statsDir , statsFile , dataset = inicializarVariables(repDir, params,ds)         \n",
    "            \n",
    "        resultados = []    \n",
    "        for run in range(params['runs']): \n",
    "            print('__________________________________________________')\n",
    "            print(f'DATASET : {ds_i}/{maxDS}')            \n",
    "            print('__________________________________________________')\n",
    "            print(f'Inicio de la prueba N°: {(run+1)}/{params[\"runs\"]}') \n",
    "            print(f'Dataset: {ds.split(\".\")[0]}')\n",
    "            print(f'- Batch size:  {params[\"batch\"]}')\n",
    "            print('__________________________________________________')\n",
    "            \n",
    "           \n",
    "            # Modelo \n",
    "            model = crearModelo(params,run) \n",
    "            metricas = getMetrics(params['redTipo'], params['lr'], params['paciencia'])\n",
    "            model.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "            \n",
    "            # Dataset        \n",
    "            train_dataset, val_dataset = splitDataset(params,run, dataset, path_imagenes, products, times, params['val_split'])\n",
    "      \n",
    "            # Creamos los callbacks\n",
    "            CB, idModel = crearCallbacks(statsDir,ds.split('/')[-1][:-4], params,run, metricas)           \n",
    "\n",
    "            if params['redTipo'] == 'Clasificacion':\n",
    "                hist =  CustomCB(val_dataset, params['WANDB'])\n",
    "                #CB.append(hist)\n",
    "        \n",
    "            \n",
    "            #Entrenamos\n",
    "            history = model.fit(train_dataset,batch_size=params['batch'],                            \n",
    "                                epochs=params['epocas'],callbacks=CB,\n",
    "                                validation_data=val_dataset,\n",
    "                                validation_batch_size=params['batch'],\n",
    "                                verbose=1)\n",
    "            if params['redTipo'] != 'Clasificacion': \n",
    "                HistTemp = history\n",
    "            else:\n",
    "                HistTemp = history#hist\n",
    "                \n",
    "            resultados.append(HistTemp.history)\n",
    "            \n",
    "            # Guardamos las estadisticas\n",
    "            if params['record']:                \n",
    "                with pd.ExcelWriter(statsFile, mode=\"a\", engine=\"openpyxl\", if_sheet_exists='overlay') as writer:                    \n",
    "                    tempDF = pd.DataFrame(HistTemp.history)\n",
    "                    if params['redTipo'] == 'Clasificacion':\n",
    "                        tempDF.columns = ['loss', 'acc', 'TP', 'TN', 'FP','FN','val_loss','val_acc','val_TP','val_TN','val_FP','val_FN']\n",
    "                    tempDF.to_excel(writer,startrow=0,\n",
    "                                    sheet_name=f'{run}-({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})-{idModel}')\n",
    "            if params['WANDB']:\n",
    "                wandb.finish()\n",
    "        \n",
    "        all_result.append(resultados)\n",
    "        \n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ce9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiegoparedes\u001b[0m (\u001b[33mtesis2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "174a6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Definimos las varibles para las iteraciones\n",
    "Los parametros que van a cambiar son:\n",
    "- Canales (products)\n",
    "- Tiempos (Minutos de las imagenes)\n",
    "- margen\n",
    "\"\"\"\n",
    "\n",
    "modelTipo = 'Clasificacion'\n",
    "\n",
    "p_train = {\n",
    "            # Reportes\n",
    "          'Proyect'  : f'{modelTipo}_DUD_MEJOR_UMB2', # TesisDiego\n",
    "          'record'   : True,  # Grabar los resultados en  excels    \n",
    "          'WANDB'    : False, # Grabar los resultados en WANDB\n",
    "    \n",
    "            # Datos del modelo\n",
    "          'redTipo'  : modelTipo, # Clasificacion / Regresion\n",
    "          'inputs'   : ['imagen','umb2'], #altura]\n",
    "          'outputs'  : 'clase',       # clase / dato\n",
    "          'num_class': 2,             # Solo se usa en el modelo de clasificacion\n",
    "          'meanMatrizImagen' : False, # True -> Usa una conv2d, caso contrario conv3d                     \n",
    "          \n",
    "        \n",
    "            # Variables del entrenamiento          \n",
    "          'lr'       : 0.001,\n",
    "          'batch'    : 32,        \n",
    "          'val_split': 1,     # ¡¡ Si dsVal existe, este valor se ignora !!\n",
    "          'epocas'   : 20,  \n",
    "          'paciencia': 15,        # 0 = No paciencia  \n",
    "    \n",
    "           # Dataset\n",
    "          'dsDir'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Entrenamiento/SplitConDA_20C_DM_V2/',  #SplitConDA_DM_DFAD_DUD\n",
    "          'dsVal'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Validacion/ClaseV2_DUD_ValidacionDS.csv',\n",
    "          'maxDS'    : -1,      #-1 = Todos\n",
    "          'dataset'  : 1,     # 1 = 100% del ds\n",
    "          'DA'       : True,  # Usaulmente para clasificacion\n",
    "                    \n",
    "           # Hiper parametros \n",
    "          'canales'  : [3],\n",
    "          'tiempos'  : [4],\n",
    "          'margen'   : [30],\n",
    "          'runs'     : 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56ff0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALDIACION : 3207\n",
      "--------------------------------------------------\n",
      "Cantidad de datasets : 2\n",
      "TRAIN: 6008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>...</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>umb1</th>\n",
       "      <th>umb2</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "      <th>DA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37890</td>\n",
       "      <td>PICHARI</td>\n",
       "      <td>47E880E2</td>\n",
       "      <td>539</td>\n",
       "      <td>772</td>\n",
       "      <td>-73.83952</td>\n",
       "      <td>-12.52219</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2021-11-06-00</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>539--772--2021-11-06-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37891</td>\n",
       "      <td>SAN PABLO</td>\n",
       "      <td>4729658E</td>\n",
       "      <td>607</td>\n",
       "      <td>799</td>\n",
       "      <td>-72.61992</td>\n",
       "      <td>-13.02506</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2021-12-14-04</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>607--799--2021-12-14-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0     nombre    codigo   XO   XA  longitud  \\\n",
       "0             0       37890    PICHARI  47E880E2  539  772 -73.83952   \n",
       "1             1       37891  SAN PABLO  4729658E  607  799 -72.61992   \n",
       "\n",
       "    latitud  altura  dato  ...    99%  75%  umb1  umb2          fecha  \\\n",
       "0 -12.52219   570.0   0.1  ...  6.319  0.0   4.2  24.0  2021-11-06-00   \n",
       "1 -13.02506  1237.0   0.2  ...  1.600  0.0   2.6   6.9  2021-12-14-04   \n",
       "\n",
       "       flag flagV2                   imagen clase  DA  \n",
       "0  M0000002    C01  539--772--2021-11-06-00     0   0  \n",
       "1  M0000002    C01  607--799--2021-12-14-04     0   0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forma del DS\n",
    "ds_files = [f'{p_train[\"dsDir\"]}{e}' for e in os.listdir(p_train[\"dsDir\"])]\n",
    "ds_files = [e for e in ds_files if '.csv' in e]\n",
    "if p_train['dsVal']:\n",
    "    dfVal = pd.read_csv(p_train['dsVal'])\n",
    "    print(f'VALDIACION : {len(dfVal)}')\n",
    "    print('--------------------------------------------------')\n",
    "print(f'Cantidad de datasets : {len(ds_files)}')  \n",
    "tempDF = pd.read_csv(ds_files[0])\n",
    "print(f'TRAIN: {len(tempDF)}')\n",
    "tempDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e75fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#resultados = trainModel(p_train,path_imagenes,path_base,products,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328e353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0498f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, \\\n",
    "    MaxPool2D, GlobalMaxPool2D, Flatten\n",
    "def build_convnet(shape=(112, 112, 3)):\n",
    "    momentum = .9\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(64, (3,3), input_shape=shape,padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    \n",
    "    #model.add(BatchNormalization(momentum=momentum))    \n",
    "    #model.add(MaxPool2D())    \n",
    "    #model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    #model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
    "    #model.add(BatchNormalization(momentum=momentum))\n",
    "    \n",
    "    # flatten...\n",
    "    model.add(GlobalMaxPool2D())\n",
    "    #model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abde6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, GRU, Dense, Dropout,LSTM\n",
    "def action_model(shape=(5, 112, 112, 3), nbout=3):\n",
    "    print(shape, nbout)\n",
    "    # Create our convnet with (112, 112, 3) input shape\n",
    "    convnet = build_convnet(shape[1:])\n",
    "    \n",
    "    # then create our final model\n",
    "    model = keras.Sequential()\n",
    "    # add the convnet with (5, 112, 112, 3) shape\n",
    "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
    "    # here, you can also use GRU or LSTM\n",
    "    model.add(LSTM(32))\n",
    "    # and finally, we make a decision network\n",
    "    model.add(Dense(256, activation='relu'))    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(3, activation='softmax'))    \n",
    "    #model.add(Dense(1,activation=tf.keras.activations.sigmoid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee70ce56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nINSHAPE=(5,112,112,3) # (5, 112, 112, 3)\\nclasses = ['M02','C02','DUD']\\nmodel = action_model(INSHAPE, len(classes))\\noptimizer = keras.optimizers.Adam(0.001)\\nmodel.compile(\\n    optimizer,\\n    'categorical_crossentropy',\\n    metrics=['acc']\\n)\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "INSHAPE=(5,112,112,3) # (5, 112, 112, 3)\n",
    "classes = ['M02','C02','DUD']\n",
    "model = action_model(INSHAPE, len(classes))\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "model.compile(\n",
    "    optimizer,\n",
    "    'categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f4dc629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 112, 112, 3) 3\n"
     ]
    }
   ],
   "source": [
    "INSHAPE=(5,112,112,3) \n",
    "classes = ['M02','C02','DUD']\n",
    "model = action_model(INSHAPE, len(classes))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    #loss=tf.keras.metrics.BinaryCrossentropy(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8a7f85d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1804518917912787e-08"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aaf73b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 0, 0, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train = np.random.rand(100,5,112,112,3)\n",
    "y_train = np.random.randint(len(classes), size=100)\n",
    "x_valid = np.random.rand(10,5,112,112,3)\n",
    "y_valid= np.random.randint(len(classes), size=10)\n",
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2663065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5, 112, 112, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6baf1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# create a \"chkp\" directory before to run that\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# because ModelCheckpoint will write models inside\u001b[39;00m\n\u001b[0;32m      4\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#keras.callbacks.ModelCheckpoint('chkp/weights.{epoch:02d}-{val_loss:.2f}.hdf5',verbose=1),\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ]\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:163\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cbk \u001b[38;5;129;01min\u001b[39;00m cbks:\n\u001b[0;32m    162\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m old_v2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=50\n",
    "# create a \"chkp\" directory before to run that\n",
    "# because ModelCheckpoint will write models inside\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(verbose=1),\n",
    "    #keras.callbacks.ModelCheckpoint('chkp/weights.{epoch:02d}-{val_loss:.2f}.hdf5',verbose=1),\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_valid,y_valid),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1ad44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728d6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
