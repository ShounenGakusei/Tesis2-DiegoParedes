{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b716303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "OBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443f8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53969cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH DEL PROYECTO \n",
    "\"\"\"\n",
    "with open('../../path_base.txt') as f:\n",
    "    path_base = f.read()\n",
    "path_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DIRECTORIOS DE LAS IMAGENES (INPUT)\n",
    "\"\"\"\n",
    "path_imagenes = 'F:/GOES/'      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#Limitamos el GPU, en caso se necesite\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1715a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo2D(p,run):    \n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    mxPool_1 = tf.keras.layers.MaxPooling2D()(conv2d_1)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(mxPool_1)\n",
    "    \n",
    "    conv2d_2 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling2D()(conv2d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)\n",
    "    \n",
    "    #conv2d_3 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    #mxPool_3 = tf.keras.layers.MaxPooling2D()(conv2d_3)\n",
    "    #dropout_3  = tf.keras.layers.Dropout(0.2)(mxPool_3)\n",
    "    \n",
    "    #conv2d_4 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_3)\n",
    "    #mxPool_4 = tf.keras.layers.MaxPooling2D()(conv2d_4)\n",
    "    #dropout_4  = tf.keras.layers.Dropout(0.2)(mxPool_4)\n",
    "    \n",
    "    conv2d_5 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv2d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2589bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo3D(p,run):        \n",
    "    # Imagen                                 (6, 30, 30, 3)\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    \n",
    "    conv3d_1 = tf.keras.layers.Conv3D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(conv3d_1)\n",
    "    \n",
    "    conv3d_2 = tf.keras.layers.Conv3D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling3D()(conv3d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)    \n",
    "\n",
    "    \n",
    "    conv3d_5 = tf.keras.layers.Conv3D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv3d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(modelType, lr, paciencia):\n",
    "    \n",
    "    if modelType == 'Clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr) \n",
    "        \n",
    "        #BinaryCrossentropy() #CategoricalCrossentropy()      \n",
    "        loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif modelType == 'Regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(params,run):\n",
    "    \n",
    "    if params['meanMatrizImagen']:\n",
    "        print(f\"Creando modelo 2D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo2D(params,run)\n",
    "    else:\n",
    "        print(f\"Creando modelo 3D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo3D(params,run)\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "926f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(img, DA):\n",
    "    if DA == 1:\n",
    "        return tf.image.flip_left_right(img)\n",
    "    elif DA == 2:\n",
    "        return tf.image.flip_up_down(img)\n",
    "    elif DA == 3:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        return tf.image.flip_up_down(img)   \n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value, p,run, path_base, products, times, DA=0):\n",
    "    # imagenData[0] = XO     # imagenData[1] = XA     # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    size = int(p['margen'][run] / 2)\n",
    "\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "        \n",
    "        if DA:\n",
    "            img_decoded = applyDA(img_decoded, item['DA'])\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if p['meanMatrizImagen']:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs']) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs']:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    if p['redTipo']=='Regresion':\n",
    "        print('es regresion')\n",
    "        return tuple(itemL), float(value)\n",
    "    else:\n",
    "        return tuple(itemL), int(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p, run, dataset, path_imagenes, products, times,val_split= 0.2):\n",
    "            \n",
    "    if p['dsVal']:\n",
    "        test = pd.read_csv(p['dsVal'])\n",
    "        if p['dataset']:\n",
    "            train =  dataset.sample(frac=p['dataset'])\n",
    "            test = test.sample(frac=p['dataset'])\n",
    "        else:\n",
    "            train =  dataset\n",
    "            \n",
    "    else:\n",
    "        # Escojemos una fraccion del dataset si se ha indicado           \n",
    "        if p['dataset']:\n",
    "            train, test = train_test_split(dataset.sample(frac=p['dataset']), test_size=val_split, shuffle=True)\n",
    "        else:\n",
    "            train, test = train_test_split(dataset, test_size=val_split, shuffle=True)\n",
    "               \n",
    "    inputsList = {}\n",
    "    inputsListTest = {}\n",
    "    # Agregamos un atributo para indicar que el dato va realizar DA\n",
    "    if p['DA']:        \n",
    "        inputsList['DA'] = train['DA'].tolist() \n",
    "        #inputsListTest['DA'] = test['DA'].tolist() \n",
    "        \n",
    "        \n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}')    \n",
    "    \n",
    "    for inp in p['inputs']:\n",
    "        inputsList[inp] = train[inp].tolist()  \n",
    "        inputsListTest[inp] = test[inp].tolist()  \n",
    "        \n",
    "       \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsListTest),test[p['outputs']].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,p['DA']))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,False))#p['DA']\n",
    "       \n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])#.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(p['batch'])#.prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5adbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDir(path, newDir):\n",
    "    try:\n",
    "        pathT = os.path.join(path, newDir)\n",
    "        os.mkdir(pathT)\n",
    "        return pathT\n",
    "    except FileExistsError:\n",
    "        return pathT\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear el directorio: {newDir}\")\n",
    "        pritn(f'Path base: {path}')\n",
    "        pritn(f'Nuevo    : {newDir}')        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c195161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializarVariables(repDir, params,ds):\n",
    "    # Leemos dataset\n",
    "    try:\n",
    "        dataset = pd.read_csv(ds)        \n",
    "        dsName = ds.split('/')[-1][:-4]\n",
    "    except:\n",
    "        print(f'No se pudo leer el dataset {ds}')\n",
    "        return None\n",
    "    \n",
    "    statsDir = crearDir(repDir, dsName)\n",
    "    excelFile = f'{statsDir}/Stats_{params[\"redTipo\"]}_{params[\"outputs\"]}_{datetime.today().strftime(\"%Y%m\")}.xlsx'\n",
    "    \n",
    "    if params['record'] and not os.path.exists(excelFile):\n",
    "        writer = pd.ExcelWriter(excelFile, engine = 'xlsxwriter')\n",
    "        keys_values = params.items()\n",
    "        strParams = {str(key): str(value) for key, value in keys_values}\n",
    "        strParams['dsName'] = dsName\n",
    "        pd.DataFrame(strParams,index=[0]).to_excel(writer, sheet_name = 'Informacion')          \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "    return statsDir, excelFile, dataset            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03005a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCM(logs):\n",
    "    lKeys = list(logs.keys())\n",
    "    \n",
    "    try:\n",
    "        TN = int(logs[[x for x in lKeys if 'val_true_negatives' in x][0]])\n",
    "        TP = int(logs[[x for x in lKeys if 'val_true_positives' in x][0]])\n",
    "        FN = int(logs[[x for x in lKeys if 'val_false_negatives' in x][0]])\n",
    "        FP = int(logs[[x for x in lKeys if 'val_false_positives' in x][0]])\n",
    "    except:\n",
    "        print(f'\\nNo se pudo leer keys para la matriz de confucion en logs : {lKeys}')\n",
    "        print(f'Se intento leer: val_true_negatives,val_true_positives, val_false_negatives y val_false_positives')\n",
    "    \n",
    "       \n",
    "    y_true =  [0]*TN + [1]*TP + [1]*FN + [0]*FP\n",
    "    _y_pred = [0]*TN + [1]*TP + [0]*FN + [1]*FP\n",
    "    \n",
    "    return TN, FP, FN, TP, np.array(y_true), np.array(_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d2f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCB(Callback):\n",
    "    \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "    def __init__(self, val_ds=None, WANDB=True):     \n",
    "        self.val_ds = val_ds  \n",
    "        self.history = {}\n",
    "        self.wandb = WANDB\n",
    "   \n",
    "   \n",
    "    def on_epoch_end(self, epoch, logs={}):  \n",
    "        TN, FP, FN, TP, y_true, _y_pred = getCM(logs)\n",
    "        \n",
    "        self.history.setdefault('loss', []).append(logs['loss']) \n",
    "        self.history.setdefault('acc', []).append(logs['acc'])  \n",
    "        self.history.setdefault('val_loss', []).append(logs['val_loss']) \n",
    "        self.history.setdefault('val_acc', []).append(logs['val_acc']) \n",
    "        \n",
    "        \n",
    "        self.history.setdefault('val_TN', []).append(TN) \n",
    "        self.history.setdefault('val_FP', []).append(FP)\n",
    "        self.history.setdefault('val_FN', []).append(FN) \n",
    "        self.history.setdefault('val_TP', []).append(TP) \n",
    "        \n",
    "            \n",
    "        if self.wandb:\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                    preds=_y_pred, y_true=y_true,\n",
    "                                    class_names=[0,1]),                   \n",
    "                       'val_TN' :TN,'val_FN' :FN,'val_TP' :TP,'val_FP' :FP,\n",
    "                       'val_acc': logs['val_acc'],'loss' : logs['loss'],\n",
    "                       'val_loss': logs['val_loss'],'acc' : logs['acc']                  \n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearCallbacks(statsDir,dsName, params,run, metricas):\n",
    "    CB = metricas['callbacks']\n",
    "    \n",
    "    idModel = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = statsDir + '/Model_{epoch:02d}_' + f'{params[\"redTipo\"]}_{params[\"outputs\"]}_{idModel}.hdf5' \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                     \n",
    "                                                     verbose=1)\n",
    "    \n",
    "    # Iniciamos WANDB\n",
    "    if params['record']:        \n",
    "        CB.append(cp_callback)   \n",
    "     \n",
    "    if params['WANDB']:\n",
    "        config = dict(learning_rate=params['lr'], epochs = params['epocas'],\n",
    "             batch_size =params['batch'],architecture=\"CNN\", \n",
    "             num_classes = params['num_class'],)\n",
    "        wandb.init(project=f'{params[\"Proyect\"]}-({params[\"redTipo\"]}-{params[\"outputs\"]}-{len(params[\"inputs\"])})',            \n",
    "                   config=config,\n",
    "                   name= f'Ex_{dsName}_({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})_{idModel}')   \n",
    "                                               \n",
    "    return CB, idModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(params,path_imagenes, path_base ,products, times):    \n",
    "        \n",
    "    if '.csv' in params[\"dsDir\"]:\n",
    "        ds_files = [params[\"dsDir\"]]\n",
    "    else:\n",
    "        ds_files = [f'{params[\"dsDir\"]}{e}' for e in os.listdir(params[\"dsDir\"])]\n",
    "        ds_files = [e for e in ds_files if '.csv' in e]\n",
    "        \n",
    "    maxDS = len(ds_files) if params['maxDS']==-1 else params['maxDS']\n",
    "    print(f'Cantidad de datasets encontrados : {len(ds_files)}')    \n",
    "    print(f'Datasets a usar : {maxDS}')    \n",
    "        \n",
    "           \n",
    "    \n",
    "    all_result = []      \n",
    "    \n",
    "    repDir = crearDir(f'{path_base}/Archivos/Resultados', params[\"redTipo\"])\n",
    "    repDir = crearDir(repDir, params[\"Proyect\"])\n",
    "    \n",
    "    print(f'DIRECTORIO BASE : {repDir}')\n",
    "    \n",
    "    # Una iteracion por cada dataset existente\n",
    "    ds_i = 0\n",
    "    \n",
    "    \n",
    "    for ds in ds_files[ds_i:maxDS+1]:\n",
    "        ds_i += 1       \n",
    "        \n",
    "        # Crear carpeta para DS y archivo de stats y el ds\n",
    "        statsDir , statsFile , dataset = inicializarVariables(repDir, params,ds)         \n",
    "            \n",
    "        resultados = []    \n",
    "        for run in range(params['runs']): \n",
    "            print('__________________________________________________')\n",
    "            print(f'DATASET : {ds_i}/{maxDS}')            \n",
    "            print('__________________________________________________')\n",
    "            print(f'Inicio de la prueba N°: {(run+1)}/{params[\"runs\"]}') \n",
    "            print(f'Dataset: {ds.split(\".\")[0]}')\n",
    "            print(f'- Batch size:  {params[\"batch\"]}')\n",
    "            print('__________________________________________________')\n",
    "            \n",
    "           \n",
    "            # Modelo \n",
    "            model = crearModelo(params,run) \n",
    "            metricas = getMetrics(params['redTipo'], params['lr'], params['paciencia'])\n",
    "            model.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "            \n",
    "            # Dataset        \n",
    "            train_dataset, val_dataset = splitDataset(params,run, dataset, path_imagenes, products, times, params['val_split'])\n",
    "      \n",
    "            # Creamos los callbacks\n",
    "            CB, idModel = crearCallbacks(statsDir,ds.split('/')[-1][:-4], params,run, metricas)           \n",
    "\n",
    "            if params['redTipo'] == 'Clasificacion':\n",
    "                hist =  CustomCB(val_dataset, params['WANDB'])\n",
    "                #CB.append(hist)\n",
    "        \n",
    "            \n",
    "            #Entrenamos\n",
    "            history = model.fit(train_dataset,batch_size=params['batch'],                            \n",
    "                                epochs=params['epocas'],callbacks=CB,\n",
    "                                validation_data=val_dataset,\n",
    "                                validation_batch_size=params['batch'],\n",
    "                                verbose=1)\n",
    "            if params['redTipo'] != 'Clasificacion': \n",
    "                HistTemp = history\n",
    "            else:\n",
    "                HistTemp = history#hist\n",
    "                \n",
    "            resultados.append(HistTemp.history)\n",
    "            \n",
    "            # Guardamos las estadisticas\n",
    "            if params['record']:                \n",
    "                with pd.ExcelWriter(statsFile, mode=\"a\", engine=\"openpyxl\", if_sheet_exists='overlay') as writer:                    \n",
    "                    tempDF = pd.DataFrame(HistTemp.history)\n",
    "                    if params['redTipo'] == 'Clasificacion':\n",
    "                        tempDF.columns = ['loss', 'acc', 'TP', 'TN', 'FP','FN','val_loss','val_acc','val_TP','val_TN','val_FP','val_FN']\n",
    "                    tempDF.to_excel(writer,startrow=0,\n",
    "                                    sheet_name=f'{run}-({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})-{idModel}')\n",
    "            if params['WANDB']:\n",
    "                wandb.finish()\n",
    "        \n",
    "        all_result.append(resultados)\n",
    "        \n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9ce9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiegoparedes\u001b[0m (\u001b[33mtesis2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcf2afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Definimos las varibles para las iteraciones\n",
    "Los parametros que van a cambiar son:\n",
    "- Canales (products)\n",
    "- Tiempos (Minutos de las imagenes)\n",
    "- margen\n",
    "\"\"\"\n",
    "\n",
    "products = ['C13','C07','C08']\n",
    "times   = ['10','20','30','40','50','00']\n",
    "\n",
    "model = 'Clasificacion'\n",
    "\n",
    "p_train = {\n",
    "            # Reportes\n",
    "          'Proyect'  : f'{model}_DUD_3D_DA_DFA', # TesisDiego\n",
    "          'record'   : True,  # Grabar los resultados en  excels    \n",
    "          'WANDB'    : False, # Grabar los resultados en WANDB\n",
    "    \n",
    "            # Datos del modelo\n",
    "          'redTipo'  : model, # Clasificacion / Regresion\n",
    "          'inputs'   : ['imagen','dato'], #altura]\n",
    "          'outputs'  : 'clase',       # clase / dato\n",
    "          'num_class': 2,             # Solo se usa en el modelo de clasificacion\n",
    "          'meanMatrizImagen' : False, # True -> Usa una conv2d, caso contrario conv3d                     \n",
    "          \n",
    "        \n",
    "            # Variables del entrenamiento          \n",
    "          'lr'       : 0.001,\n",
    "          'batch'    : 32,        \n",
    "          'val_split': 1,     # ¡¡ Si dsVal existe, este valor se ignora !!\n",
    "          'epocas'   : 20,  \n",
    "          'paciencia': 15,        # 0 = No paciencia  \n",
    "    \n",
    "           # Dataset\n",
    "          'dsDir'    : f'{path_base}/Archivos/Dataset/{model}/Entrenamiento/SplitConDA_DFA_DUD/',  \n",
    "          'dsVal'    : f'{path_base}/Archivos/Dataset/{model}/Validacion/ClaseV2_DUD_ValidacionDS.csv',\n",
    "          'maxDS'    : -1,      #-1 = Todos\n",
    "          'dataset'  : 1,     # 1 = 100% del ds\n",
    "          'DA'       : True,  # Usaulmente para clasificacion\n",
    "                    \n",
    "           # Hiper parametros \n",
    "          'canales'  : [3],\n",
    "          'tiempos'  : [4],\n",
    "          'margen'   : [30],\n",
    "          'runs'     : 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "abc25a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALDIACION : 3207\n",
      "--------------------------------------------------\n",
      "Cantidad de datasets : 2\n",
      "TRAIN: 34652\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>umb1</th>\n",
       "      <th>umb2</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "      <th>DA</th>\n",
       "      <th>DFA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LARAQUERI</td>\n",
       "      <td>472CB426</td>\n",
       "      <td>750</td>\n",
       "      <td>972</td>\n",
       "      <td>-70.04589</td>\n",
       "      <td>-16.13592</td>\n",
       "      <td>3928.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>11.1</td>\n",
       "      <td>2021-12-10-04</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>D01</td>\n",
       "      <td>750--972--2021-12-10-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ANCOAQUE</td>\n",
       "      <td>117059</td>\n",
       "      <td>755</td>\n",
       "      <td>1031</td>\n",
       "      <td>-69.93917</td>\n",
       "      <td>-17.19972</td>\n",
       "      <td>4422.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2021-11-27-05</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>D02</td>\n",
       "      <td>755--1031--2021-11-27-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0     nombre    codigo   XO    XA  longitud  \\\n",
       "0             0           0  LARAQUERI  472CB426  750   972 -70.04589   \n",
       "1             1           1   ANCOAQUE    117059  755  1031 -69.93917   \n",
       "\n",
       "    latitud  altura  dato  ...  75%  umb1  umb2          fecha      flag  \\\n",
       "0 -16.13592  3928.0   0.5  ...  0.0   3.6  11.1  2021-12-10-04  C0000002   \n",
       "1 -17.19972  4422.0   0.1  ...  0.0   1.6   3.9  2021-11-27-05  C0000002   \n",
       "\n",
       "  flagV2                    imagen clase DA  DFA  \n",
       "0    D01   750--972--2021-12-10-04     1  0    0  \n",
       "1    D02  755--1031--2021-11-27-05     1  0    0  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forma del DS\n",
    "ds_files = [f'{p_train[\"dsDir\"]}{e}' for e in os.listdir(p_train[\"dsDir\"])]\n",
    "ds_files = [e for e in ds_files if '.csv' in e]\n",
    "if p_train['dsVal']:\n",
    "    dfVal = pd.read_csv(p_train['dsVal'])\n",
    "    print(f'VALDIACION : {len(dfVal)}')\n",
    "    print('--------------------------------------------------')\n",
    "print(f'Cantidad de datasets : {len(ds_files)}')  \n",
    "tempDF = pd.read_csv(ds_files[0])\n",
    "print(f'TRAIN: {len(tempDF)}')\n",
    "tempDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b2ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datasets encontrados : 2\n",
      "Datasets a usar : -1\n",
      "DIRECTORIO BASE : C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_3D_DA_DFA\n",
      "__________________________________________________\n",
      "DATASET : 1/-1\n",
      "__________________________________________________\n",
      "Inicio de la prueba N°: 1/1\n",
      "Dataset: C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Dataset/Clasificacion/Entrenamiento/SplitConDA_DFA_DUD/CLASE_TrainDS_0\n",
      "- Batch size:  32\n",
      "__________________________________________________\n",
      "Creando modelo 3D\n",
      "HP :(T:4 - M:30 - C:3) y  tipo (Clasificacion)\n",
      "Tamaño del dataset: Train 34652  - Val 3207\n",
      "Epoch 1/20\n",
      "1083/1083 [==============================] - ETA: 0s - loss: 0.1782 - acc: 0.9511 - true_positives_41: 17272.0000 - true_negatives_41: 15687.0000 - false_positives_41: 1639.0000 - false_negatives_41: 54.0000\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_3D_DA_DFA\\CLASE_TrainDS_0\\Model_01_Clasificacion_clase_20220527_113052.hdf5\n",
      "1083/1083 [==============================] - 1068s 985ms/step - loss: 0.1782 - acc: 0.9511 - true_positives_41: 17272.0000 - true_negatives_41: 15687.0000 - false_positives_41: 1639.0000 - false_negatives_41: 54.0000 - val_loss: 0.4328 - val_acc: 0.8946 - val_true_positives_41: 2860.0000 - val_true_negatives_41: 9.0000 - val_false_positives_41: 41.0000 - val_false_negatives_41: 297.0000\n",
      "Epoch 2/20\n",
      " 616/1083 [================>.............] - ETA: 7:52 - loss: 0.1501 - acc: 0.9566 - true_positives_41: 9845.0000 - true_negatives_41: 9011.0000 - false_positives_41: 848.0000 - false_negatives_41: 8.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultados = trainModel(p_train,path_imagenes,path_base,products,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694db9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
