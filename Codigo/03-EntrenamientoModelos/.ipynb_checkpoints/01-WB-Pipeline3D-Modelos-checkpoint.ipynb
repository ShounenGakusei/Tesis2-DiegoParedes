{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b716303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \n",
    "\"\"\"\n",
    "Autor='Diego Paredes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443f8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53969cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH DEL PROYECTO \n",
    "\"\"\"\n",
    "with open('../../path_base.txt') as f:\n",
    "    path_base = f.read()\n",
    "path_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables generales\n",
    "\"\"\"\n",
    "path_imagenes = 'F:/GOES/'     \n",
    "\n",
    "products = ['C13','C07','C08']\n",
    "times   = ['10','20','30','40','50','00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#Limitamos el GPU, en caso se necesite\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1715a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo2D(p,run):    \n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    mxPool_1 = tf.keras.layers.MaxPooling2D()(conv2d_1)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(mxPool_1)\n",
    "    \n",
    "    conv2d_2 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling2D()(conv2d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)\n",
    "    \n",
    "    #conv2d_3 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    #mxPool_3 = tf.keras.layers.MaxPooling2D()(conv2d_3)\n",
    "    #dropout_3  = tf.keras.layers.Dropout(0.2)(mxPool_3)\n",
    "    \n",
    "    #conv2d_4 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_3)\n",
    "    #mxPool_4 = tf.keras.layers.MaxPooling2D()(conv2d_4)\n",
    "    #dropout_4  = tf.keras.layers.Dropout(0.2)(mxPool_4)\n",
    "    \n",
    "    conv2d_5 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv2d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2589bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo3D(p,run):        \n",
    "    # Imagen                                 (6, 30, 30, 3)\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    \n",
    "    conv3d_1 = tf.keras.layers.Conv3D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(conv3d_1)\n",
    "    \n",
    "    conv3d_2 = tf.keras.layers.Conv3D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling3D()(conv3d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)    \n",
    "\n",
    "    \n",
    "    conv3d_5 = tf.keras.layers.Conv3D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv3d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(modelType, lr, paciencia):\n",
    "    \n",
    "    if modelType == 'Clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr) \n",
    "        \n",
    "        #BinaryCrossentropy() #CategoricalCrossentropy()      \n",
    "        loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif modelType == 'Regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(params,run):\n",
    "    \n",
    "    if params['meanMatrizImagen']:\n",
    "        print(f\"Creando modelo 2D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo2D(params,run)\n",
    "    else:\n",
    "        print(f\"Creando modelo 3D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo3D(params,run)\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(img, DA):\n",
    "    if DA == 1:\n",
    "        return tf.image.flip_left_right(img)\n",
    "    elif DA == 2:\n",
    "        return tf.image.flip_up_down(img)\n",
    "    elif DA == 3:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        return tf.image.flip_up_down(img)   \n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value, p,run, path_base, products, times, DA=0):\n",
    "    # imagenData[0] = XO     # imagenData[1] = XA     # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    size = int(p['margen'][run] / 2)\n",
    "\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "        \n",
    "        if DA:\n",
    "            img_decoded = applyDA(img_decoded, item['DA'])\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if p['meanMatrizImagen']:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs']) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs']:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    if p['redTipo']=='Regresion':\n",
    "        print('es regresion')\n",
    "        return tuple(itemL), float(value)\n",
    "    else:\n",
    "        return tuple(itemL), int(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p, run, dataset, path_imagenes, products, times,val_split= 0.2):\n",
    "            \n",
    "    if p['dsVal']:\n",
    "        test = pd.read_csv(p['dsVal'])\n",
    "        if p['dataset']:\n",
    "            train =  dataset.sample(frac=p['dataset'])\n",
    "            test = test.sample(frac=p['dataset'])\n",
    "        else:\n",
    "            train =  dataset\n",
    "            \n",
    "    else:\n",
    "        # Escojemos una fraccion del dataset si se ha indicado           \n",
    "        if p['dataset']:\n",
    "            train, test = train_test_split(dataset.sample(frac=p['dataset']), test_size=val_split, shuffle=True)\n",
    "        else:\n",
    "            train, test = train_test_split(dataset, test_size=val_split, shuffle=True)\n",
    "               \n",
    "    inputsList = {}\n",
    "    inputsListTest = {}\n",
    "    # Agregamos un atributo para indicar que el dato va realizar DA\n",
    "    if p['DA']:        \n",
    "        inputsList['DA'] = train['DA'].tolist() \n",
    "        #inputsListTest['DA'] = test['DA'].tolist() \n",
    "        \n",
    "        \n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}')    \n",
    "    \n",
    "    for inp in p['inputs']:\n",
    "        inputsList[inp] = train[inp].tolist()  \n",
    "        inputsListTest[inp] = test[inp].tolist()  \n",
    "        \n",
    "       \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsListTest),test[p['outputs']].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,p['DA']))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,False))#p['DA']\n",
    "       \n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])#.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(p['batch'])#.prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5adbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDir(path, newDir):\n",
    "    try:\n",
    "        pathT = os.path.join(path, newDir)\n",
    "        os.mkdir(pathT)\n",
    "        return pathT\n",
    "    except FileExistsError:\n",
    "        return pathT\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear el directorio: {newDir}\")\n",
    "        pritn(f'Path base: {path}')\n",
    "        pritn(f'Nuevo    : {newDir}')        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c195161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializarVariables(repDir, params,ds):\n",
    "    # Leemos dataset\n",
    "    try:\n",
    "        dataset = pd.read_csv(ds)        \n",
    "        dsName = ds.split('/')[-1][:-4]\n",
    "    except:\n",
    "        print(f'No se pudo leer el dataset {ds}')\n",
    "        return None\n",
    "    \n",
    "    statsDir = crearDir(repDir, dsName)\n",
    "    excelFile = f'{statsDir}/Stats_{params[\"redTipo\"]}_{params[\"outputs\"]}_{datetime.today().strftime(\"%Y%m\")}.xlsx'\n",
    "    \n",
    "    if params['record'] and not os.path.exists(excelFile):\n",
    "        writer = pd.ExcelWriter(excelFile, engine = 'xlsxwriter')\n",
    "        keys_values = params.items()\n",
    "        strParams = {str(key): str(value) for key, value in keys_values}\n",
    "        strParams['dsName'] = dsName\n",
    "        pd.DataFrame(strParams,index=[0]).to_excel(writer, sheet_name = 'Informacion')          \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "    return statsDir, excelFile, dataset            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03005a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCM(logs):\n",
    "    lKeys = list(logs.keys())\n",
    "    \n",
    "    try:\n",
    "        TN = int(logs[[x for x in lKeys if 'val_true_negatives' in x][0]])\n",
    "        TP = int(logs[[x for x in lKeys if 'val_true_positives' in x][0]])\n",
    "        FN = int(logs[[x for x in lKeys if 'val_false_negatives' in x][0]])\n",
    "        FP = int(logs[[x for x in lKeys if 'val_false_positives' in x][0]])\n",
    "    except:\n",
    "        print(f'\\nNo se pudo leer keys para la matriz de confucion en logs : {lKeys}')\n",
    "        print(f'Se intento leer: val_true_negatives,val_true_positives, val_false_negatives y val_false_positives')\n",
    "    \n",
    "       \n",
    "    y_true =  [0]*TN + [1]*TP + [1]*FN + [0]*FP\n",
    "    _y_pred = [0]*TN + [1]*TP + [0]*FN + [1]*FP\n",
    "    \n",
    "    return TN, FP, FN, TP, np.array(y_true), np.array(_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d2f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCB(Callback):\n",
    "    \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "    def __init__(self, val_ds=None, WANDB=True):     \n",
    "        self.val_ds = val_ds  \n",
    "        self.history = {}\n",
    "        self.wandb = WANDB\n",
    "   \n",
    "   \n",
    "    def on_epoch_end(self, epoch, logs={}):  \n",
    "        TN, FP, FN, TP, y_true, _y_pred = getCM(logs)\n",
    "        \n",
    "        self.history.setdefault('loss', []).append(logs['loss']) \n",
    "        self.history.setdefault('acc', []).append(logs['acc'])  \n",
    "        self.history.setdefault('val_loss', []).append(logs['val_loss']) \n",
    "        self.history.setdefault('val_acc', []).append(logs['val_acc']) \n",
    "        \n",
    "        \n",
    "        self.history.setdefault('val_TN', []).append(TN) \n",
    "        self.history.setdefault('val_FP', []).append(FP)\n",
    "        self.history.setdefault('val_FN', []).append(FN) \n",
    "        self.history.setdefault('val_TP', []).append(TP) \n",
    "        \n",
    "            \n",
    "        if self.wandb:\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                    preds=_y_pred, y_true=y_true,\n",
    "                                    class_names=[0,1]),                   \n",
    "                       'val_TN' :TN,'val_FN' :FN,'val_TP' :TP,'val_FP' :FP,\n",
    "                       'val_acc': logs['val_acc'],'loss' : logs['loss'],\n",
    "                       'val_loss': logs['val_loss'],'acc' : logs['acc']                  \n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearCallbacks(statsDir,dsName, params,run, metricas):\n",
    "    CB = metricas['callbacks']\n",
    "    \n",
    "    idModel = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = statsDir + '/Model_{epoch:02d}_' + f'{params[\"redTipo\"]}_{params[\"outputs\"]}_{idModel}.hdf5' \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                     \n",
    "                                                     verbose=1)\n",
    "    \n",
    "    # Iniciamos WANDB\n",
    "    if params['record']:        \n",
    "        CB.append(cp_callback)   \n",
    "     \n",
    "    if params['WANDB']:\n",
    "        config = dict(learning_rate=params['lr'], epochs = params['epocas'],\n",
    "             batch_size =params['batch'],architecture=\"CNN\", \n",
    "             num_classes = params['num_class'],)\n",
    "        wandb.init(project=f'{params[\"Proyect\"]}-({params[\"redTipo\"]}-{params[\"outputs\"]}-{len(params[\"inputs\"])})',            \n",
    "                   config=config,\n",
    "                   name= f'Ex_{dsName}_({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})_{idModel}')   \n",
    "                                               \n",
    "    return CB, idModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(params,path_imagenes, path_base ,products, times):   \n",
    "    idProject = datetime.today().strftime(\"%Y%m%d_%H\")\n",
    "    params['Proyect'] = f'{params['Proyect']}-{idProject}'\n",
    "        \n",
    "    if '.csv' in params[\"dsDir\"]:\n",
    "        ds_files = [params[\"dsDir\"]]\n",
    "    else:\n",
    "        ds_files = [f'{params[\"dsDir\"]}{e}' for e in os.listdir(params[\"dsDir\"])]\n",
    "        ds_files = [e for e in ds_files if '.csv' in e]\n",
    "        \n",
    "    maxDS = len(ds_files) if params['maxDS']==-1 else params['maxDS']\n",
    "    print(f'Cantidad de datasets encontrados : {len(ds_files)}')    \n",
    "    print(f'Datasets a usar : {maxDS}')    \n",
    "        \n",
    "           \n",
    "    \n",
    "    all_result = []      \n",
    "    \n",
    "    repDir = crearDir(f'{path_base}/Archivos/Resultados', params[\"redTipo\"])\n",
    "    repDir = crearDir(repDir, params[\"Proyect\"])\n",
    "    \n",
    "    print(f'DIRECTORIO BASE : {repDir}')\n",
    "    \n",
    "    # Una iteracion por cada dataset existente\n",
    "    ds_i = 0\n",
    "    \n",
    "    \n",
    "    for ds in ds_files[ds_i:maxDS+1]:\n",
    "        ds_i += 1       \n",
    "        \n",
    "        # Crear carpeta para DS y archivo de stats y el ds\n",
    "        statsDir , statsFile , dataset = inicializarVariables(repDir, params,ds)         \n",
    "            \n",
    "        resultados = []    \n",
    "        for run in range(params['runs']): \n",
    "            print('__________________________________________________')\n",
    "            print(f'DATASET : {ds_i}/{maxDS}')            \n",
    "            print('__________________________________________________')\n",
    "            print(f'Inicio de la prueba N°: {(run+1)}/{params[\"runs\"]}') \n",
    "            print(f'Dataset: {ds.split(\".\")[0]}')\n",
    "            print(f'- Batch size:  {params[\"batch\"]}')\n",
    "            print('__________________________________________________')\n",
    "            \n",
    "           \n",
    "            # Modelo \n",
    "            model = crearModelo(params,run) \n",
    "            metricas = getMetrics(params['redTipo'], params['lr'], params['paciencia'])\n",
    "            model.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "            \n",
    "            # Dataset        \n",
    "            train_dataset, val_dataset = splitDataset(params,run, dataset, path_imagenes, products, times, params['val_split'])\n",
    "      \n",
    "            # Creamos los callbacks\n",
    "            CB, idModel = crearCallbacks(statsDir,ds.split('/')[-1][:-4], params,run, metricas)           \n",
    "\n",
    "            if params['redTipo'] == 'Clasificacion':\n",
    "                hist =  CustomCB(val_dataset, params['WANDB'])\n",
    "                #CB.append(hist)\n",
    "        \n",
    "            \n",
    "            #Entrenamos\n",
    "            history = model.fit(train_dataset,batch_size=params['batch'],                            \n",
    "                                epochs=params['epocas'],callbacks=CB,\n",
    "                                validation_data=val_dataset,\n",
    "                                validation_batch_size=params['batch'],\n",
    "                                verbose=1)\n",
    "            if params['redTipo'] != 'Clasificacion': \n",
    "                HistTemp = history\n",
    "            else:\n",
    "                HistTemp = history#hist\n",
    "                \n",
    "            resultados.append(HistTemp.history)\n",
    "            \n",
    "            # Guardamos las estadisticas\n",
    "            if params['record']:                \n",
    "                with pd.ExcelWriter(statsFile, mode=\"a\", engine=\"openpyxl\", if_sheet_exists='overlay') as writer:                    \n",
    "                    tempDF = pd.DataFrame(HistTemp.history)\n",
    "                    if params['redTipo'] == 'Clasificacion':\n",
    "                        tempDF.columns = ['loss', 'acc', 'TP', 'TN', 'FP','FN','val_loss','val_acc','val_TP','val_TN','val_FP','val_FN']\n",
    "                    tempDF.to_excel(writer,startrow=0,\n",
    "                                    sheet_name=f'{run}-({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})-{idModel}')\n",
    "            if params['WANDB']:\n",
    "                wandb.finish()\n",
    "        \n",
    "        all_result.append(resultados)\n",
    "        \n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9ce9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiegoparedes\u001b[0m (\u001b[33mtesis2\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "174a6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Definimos las varibles para las iteraciones\n",
    "Los parametros que van a cambiar son:\n",
    "- Canales (products)\n",
    "- Tiempos (Minutos de las imagenes)\n",
    "- margen\n",
    "\"\"\"\n",
    "\n",
    "modelTipo = 'Clasificacion'\n",
    "\n",
    "p_train = {\n",
    "            # Reportes\n",
    "          'Proyect'  : f'{modelTipo}_DUD_MEJOR_DATO', # TesisDiego\n",
    "          'record'   : True,  # Grabar los resultados en  excels    \n",
    "          'WANDB'    : False, # Grabar los resultados en WANDB\n",
    "    \n",
    "            # Datos del modelo\n",
    "          'redTipo'  : modelTipo, # Clasificacion / Regresion\n",
    "          'inputs'   : ['imagen','dato'], #altura]\n",
    "          'outputs'  : 'clase',       # clase / dato\n",
    "          'num_class': 2,             # Solo se usa en el modelo de clasificacion\n",
    "          'meanMatrizImagen' : False, # True -> Usa una conv2d, caso contrario conv3d                     \n",
    "          \n",
    "        \n",
    "            # Variables del entrenamiento          \n",
    "          'lr'       : 0.001,\n",
    "          'batch'    : 32,        \n",
    "          'val_split': 1,     # ¡¡ Si dsVal existe, este valor se ignora !!\n",
    "          'epocas'   : 20,  \n",
    "          'paciencia': 15,        # 0 = No paciencia  \n",
    "    \n",
    "           # Dataset\n",
    "          'dsDir'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Entrenamiento/SplitConDA_20C_DM_V2/',  #SplitConDA_DM_DFAD_DUD\n",
    "          'dsVal'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Validacion/ClaseV2_DUD_ValidacionDS.csv',\n",
    "          'maxDS'    : -1,      #-1 = Todos\n",
    "          'dataset'  : 1,     # 1 = 100% del ds\n",
    "          'DA'       : True,  # Usaulmente para clasificacion\n",
    "                    \n",
    "           # Hiper parametros \n",
    "          'canales'  : [3],\n",
    "          'tiempos'  : [4],\n",
    "          'margen'   : [30],\n",
    "          'runs'     : 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56ff0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALDIACION : 3207\n",
      "--------------------------------------------------\n",
      "Cantidad de datasets : 2\n",
      "TRAIN: 6008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>...</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>umb1</th>\n",
       "      <th>umb2</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "      <th>DA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37890</td>\n",
       "      <td>PICHARI</td>\n",
       "      <td>47E880E2</td>\n",
       "      <td>539</td>\n",
       "      <td>772</td>\n",
       "      <td>-73.83952</td>\n",
       "      <td>-12.52219</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2021-11-06-00</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>539--772--2021-11-06-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37891</td>\n",
       "      <td>SAN PABLO</td>\n",
       "      <td>4729658E</td>\n",
       "      <td>607</td>\n",
       "      <td>799</td>\n",
       "      <td>-72.61992</td>\n",
       "      <td>-13.02506</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2021-12-14-04</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>607--799--2021-12-14-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0     nombre    codigo   XO   XA  longitud  \\\n",
       "0             0       37890    PICHARI  47E880E2  539  772 -73.83952   \n",
       "1             1       37891  SAN PABLO  4729658E  607  799 -72.61992   \n",
       "\n",
       "    latitud  altura  dato  ...    99%  75%  umb1  umb2          fecha  \\\n",
       "0 -12.52219   570.0   0.1  ...  6.319  0.0   4.2  24.0  2021-11-06-00   \n",
       "1 -13.02506  1237.0   0.2  ...  1.600  0.0   2.6   6.9  2021-12-14-04   \n",
       "\n",
       "       flag flagV2                   imagen clase  DA  \n",
       "0  M0000002    C01  539--772--2021-11-06-00     0   0  \n",
       "1  M0000002    C01  607--799--2021-12-14-04     0   0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forma del DS\n",
    "ds_files = [f'{p_train[\"dsDir\"]}{e}' for e in os.listdir(p_train[\"dsDir\"])]\n",
    "ds_files = [e for e in ds_files if '.csv' in e]\n",
    "if p_train['dsVal']:\n",
    "    dfVal = pd.read_csv(p_train['dsVal'])\n",
    "    print(f'VALDIACION : {len(dfVal)}')\n",
    "    print('--------------------------------------------------')\n",
    "print(f'Cantidad de datasets : {len(ds_files)}')  \n",
    "tempDF = pd.read_csv(ds_files[0])\n",
    "print(f'TRAIN: {len(tempDF)}')\n",
    "tempDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datasets encontrados : 2\n",
      "Datasets a usar : 2\n",
      "DIRECTORIO BASE : C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\n",
      "__________________________________________________\n",
      "DATASET : 1/2\n",
      "__________________________________________________\n",
      "Inicio de la prueba N°: 1/1\n",
      "Dataset: C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Dataset/Clasificacion/Entrenamiento/SplitConDA_20C_DM_V2/CLASE_TrainDS_0\n",
      "- Batch size:  32\n",
      "__________________________________________________\n",
      "Creando modelo 3D\n",
      "HP :(T:4 - M:30 - C:3) y  tipo (Clasificacion)\n",
      "Tamaño del dataset: Train 6008  - Val 3207\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6764 - acc: 0.5869 - true_positives_2: 1421.0000 - true_negatives_2: 2105.0000 - false_positives_2: 899.0000 - false_negatives_2: 1583.0000\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_01_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 217s 1s/step - loss: 0.6764 - acc: 0.5869 - true_positives_2: 1421.0000 - true_negatives_2: 2105.0000 - false_positives_2: 899.0000 - false_negatives_2: 1583.0000 - val_loss: 0.5937 - val_acc: 0.9835 - val_true_positives_2: 3152.0000 - val_true_negatives_2: 2.0000 - val_false_positives_2: 48.0000 - val_false_negatives_2: 5.0000\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6336 - acc: 0.6661 - true_positives_2: 1768.0000 - true_negatives_2: 2234.0000 - false_positives_2: 770.0000 - false_negatives_2: 1236.0000\n",
      "Epoch 2: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_02_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 218s 1s/step - loss: 0.6336 - acc: 0.6661 - true_positives_2: 1768.0000 - true_negatives_2: 2234.0000 - false_positives_2: 770.0000 - false_negatives_2: 1236.0000 - val_loss: 0.6327 - val_acc: 0.5145 - val_true_positives_2: 1615.0000 - val_true_negatives_2: 35.0000 - val_false_positives_2: 15.0000 - val_false_negatives_2: 1542.0000\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6028 - acc: 0.6924 - true_positives_2: 1748.0000 - true_negatives_2: 2412.0000 - false_positives_2: 592.0000 - false_negatives_2: 1256.0000\n",
      "Epoch 3: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_03_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 266s 1s/step - loss: 0.6028 - acc: 0.6924 - true_positives_2: 1748.0000 - true_negatives_2: 2412.0000 - false_positives_2: 592.0000 - false_negatives_2: 1256.0000 - val_loss: 0.6137 - val_acc: 0.5625 - val_true_positives_2: 1772.0000 - val_true_negatives_2: 32.0000 - val_false_positives_2: 18.0000 - val_false_negatives_2: 1385.0000\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5871 - acc: 0.7059 - true_positives_2: 1823.0000 - true_negatives_2: 2418.0000 - false_positives_2: 586.0000 - false_negatives_2: 1181.0000\n",
      "Epoch 4: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_04_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 297s 2s/step - loss: 0.5871 - acc: 0.7059 - true_positives_2: 1823.0000 - true_negatives_2: 2418.0000 - false_positives_2: 586.0000 - false_negatives_2: 1181.0000 - val_loss: 0.6181 - val_acc: 0.5968 - val_true_positives_2: 1883.0000 - val_true_negatives_2: 31.0000 - val_false_positives_2: 19.0000 - val_false_negatives_2: 1274.0000\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5808 - acc: 0.7145 - true_positives_2: 1906.0000 - true_negatives_2: 2387.0000 - false_positives_2: 617.0000 - false_negatives_2: 1098.0000\n",
      "Epoch 5: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_05_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 219s 1s/step - loss: 0.5808 - acc: 0.7145 - true_positives_2: 1906.0000 - true_negatives_2: 2387.0000 - false_positives_2: 617.0000 - false_negatives_2: 1098.0000 - val_loss: 0.5242 - val_acc: 0.6517 - val_true_positives_2: 2060.0000 - val_true_negatives_2: 30.0000 - val_false_positives_2: 20.0000 - val_false_negatives_2: 1097.0000\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5746 - acc: 0.7164 - true_positives_2: 1941.0000 - true_negatives_2: 2363.0000 - false_positives_2: 641.0000 - false_negatives_2: 1063.0000\n",
      "Epoch 6: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_06_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 221s 1s/step - loss: 0.5746 - acc: 0.7164 - true_positives_2: 1941.0000 - true_negatives_2: 2363.0000 - false_positives_2: 641.0000 - false_negatives_2: 1063.0000 - val_loss: 0.5810 - val_acc: 0.6517 - val_true_positives_2: 2060.0000 - val_true_negatives_2: 30.0000 - val_false_positives_2: 20.0000 - val_false_negatives_2: 1097.0000\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5720 - acc: 0.7222 - true_positives_2: 1989.0000 - true_negatives_2: 2350.0000 - false_positives_2: 654.0000 - false_negatives_2: 1015.0000\n",
      "Epoch 7: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_07_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 219s 1s/step - loss: 0.5720 - acc: 0.7222 - true_positives_2: 1989.0000 - true_negatives_2: 2350.0000 - false_positives_2: 654.0000 - false_negatives_2: 1015.0000 - val_loss: 0.5627 - val_acc: 0.6517 - val_true_positives_2: 2060.0000 - val_true_negatives_2: 30.0000 - val_false_positives_2: 20.0000 - val_false_negatives_2: 1097.0000\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5699 - acc: 0.7260 - true_positives_2: 2033.0000 - true_negatives_2: 2329.0000 - false_positives_2: 675.0000 - false_negatives_2: 971.0000\n",
      "Epoch 8: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_08_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 326s 2s/step - loss: 0.5699 - acc: 0.7260 - true_positives_2: 2033.0000 - true_negatives_2: 2329.0000 - false_positives_2: 675.0000 - false_negatives_2: 971.0000 - val_loss: 0.5491 - val_acc: 0.6517 - val_true_positives_2: 2060.0000 - val_true_negatives_2: 30.0000 - val_false_positives_2: 20.0000 - val_false_negatives_2: 1097.0000\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5686 - acc: 0.7277 - true_positives_2: 2058.0000 - true_negatives_2: 2314.0000 - false_positives_2: 690.0000 - false_negatives_2: 946.0000\n",
      "Epoch 9: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_09_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 251s 1s/step - loss: 0.5686 - acc: 0.7277 - true_positives_2: 2058.0000 - true_negatives_2: 2314.0000 - false_positives_2: 690.0000 - false_negatives_2: 946.0000 - val_loss: 0.5400 - val_acc: 0.7122 - val_true_positives_2: 2255.0000 - val_true_negatives_2: 29.0000 - val_false_positives_2: 21.0000 - val_false_negatives_2: 902.0000\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5670 - acc: 0.7282 - true_positives_2: 2071.0000 - true_negatives_2: 2304.0000 - false_positives_2: 700.0000 - false_negatives_2: 933.0000\n",
      "Epoch 10: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_10_Clasificacion_clase_20220601_150950.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 343s 2s/step - loss: 0.5670 - acc: 0.7282 - true_positives_2: 2071.0000 - true_negatives_2: 2304.0000 - false_positives_2: 700.0000 - false_negatives_2: 933.0000 - val_loss: 0.5323 - val_acc: 0.7122 - val_true_positives_2: 2255.0000 - val_true_negatives_2: 29.0000 - val_false_positives_2: 21.0000 - val_false_negatives_2: 902.0000\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5658 - acc: 0.7307 - true_positives_2: 2101.0000 - true_negatives_2: 2289.0000 - false_positives_2: 715.0000 - false_negatives_2: 903.0000\n",
      "Epoch 11: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_11_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 252s 1s/step - loss: 0.5658 - acc: 0.7307 - true_positives_2: 2101.0000 - true_negatives_2: 2289.0000 - false_positives_2: 715.0000 - false_negatives_2: 903.0000 - val_loss: 0.5300 - val_acc: 0.7122 - val_true_positives_2: 2255.0000 - val_true_negatives_2: 29.0000 - val_false_positives_2: 21.0000 - val_false_negatives_2: 902.0000\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5650 - acc: 0.7309 - true_positives_2: 2108.0000 - true_negatives_2: 2283.0000 - false_positives_2: 721.0000 - false_negatives_2: 896.0000\n",
      "Epoch 12: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_12_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 234s 1s/step - loss: 0.5650 - acc: 0.7309 - true_positives_2: 2108.0000 - true_negatives_2: 2283.0000 - false_positives_2: 721.0000 - false_negatives_2: 896.0000 - val_loss: 0.5277 - val_acc: 0.7122 - val_true_positives_2: 2255.0000 - val_true_negatives_2: 29.0000 - val_false_positives_2: 21.0000 - val_false_negatives_2: 902.0000\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5648 - acc: 0.7304 - true_positives_2: 2112.0000 - true_negatives_2: 2276.0000 - false_positives_2: 728.0000 - false_negatives_2: 892.0000\n",
      "Epoch 13: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_13_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 230s 1s/step - loss: 0.5648 - acc: 0.7304 - true_positives_2: 2112.0000 - true_negatives_2: 2276.0000 - false_positives_2: 728.0000 - false_negatives_2: 892.0000 - val_loss: 0.5241 - val_acc: 0.7159 - val_true_positives_2: 2269.0000 - val_true_negatives_2: 27.0000 - val_false_positives_2: 23.0000 - val_false_negatives_2: 888.0000\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5648 - acc: 0.7302 - true_positives_2: 2127.0000 - true_negatives_2: 2260.0000 - false_positives_2: 744.0000 - false_negatives_2: 877.0000\n",
      "Epoch 14: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_14_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 233s 1s/step - loss: 0.5648 - acc: 0.7302 - true_positives_2: 2127.0000 - true_negatives_2: 2260.0000 - false_positives_2: 744.0000 - false_negatives_2: 877.0000 - val_loss: 0.5205 - val_acc: 0.7159 - val_true_positives_2: 2269.0000 - val_true_negatives_2: 27.0000 - val_false_positives_2: 23.0000 - val_false_negatives_2: 888.0000\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5645 - acc: 0.7305 - true_positives_2: 2130.0000 - true_negatives_2: 2259.0000 - false_positives_2: 745.0000 - false_negatives_2: 874.0000\n",
      "Epoch 15: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_15_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 235s 1s/step - loss: 0.5645 - acc: 0.7305 - true_positives_2: 2130.0000 - true_negatives_2: 2259.0000 - false_positives_2: 745.0000 - false_negatives_2: 874.0000 - val_loss: 0.5177 - val_acc: 0.7159 - val_true_positives_2: 2269.0000 - val_true_negatives_2: 27.0000 - val_false_positives_2: 23.0000 - val_false_negatives_2: 888.0000\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5643 - acc: 0.7297 - true_positives_2: 2127.0000 - true_negatives_2: 2257.0000 - false_positives_2: 747.0000 - false_negatives_2: 877.0000\n",
      "Epoch 16: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_0\\Model_16_Clasificacion_clase_20220601_150950.hdf5\n",
      "188/188 [==============================] - 234s 1s/step - loss: 0.5643 - acc: 0.7297 - true_positives_2: 2127.0000 - true_negatives_2: 2257.0000 - false_positives_2: 747.0000 - false_negatives_2: 877.0000 - val_loss: 0.5153 - val_acc: 0.7159 - val_true_positives_2: 2269.0000 - val_true_negatives_2: 27.0000 - val_false_positives_2: 23.0000 - val_false_negatives_2: 888.0000\n",
      "__________________________________________________\n",
      "DATASET : 2/2\n",
      "__________________________________________________\n",
      "Inicio de la prueba N°: 1/1\n",
      "Dataset: C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Dataset/Clasificacion/Entrenamiento/SplitConDA_20C_DM_V2/CLASE_TrainDS_1\n",
      "- Batch size:  32\n",
      "__________________________________________________\n",
      "Creando modelo 3D\n",
      "HP :(T:4 - M:30 - C:3) y  tipo (Clasificacion)\n",
      "Tamaño del dataset: Train 6008  - Val 3207\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6806 - acc: 0.5619 - true_positives_3: 1961.0000 - true_negatives_3: 1415.0000 - false_positives_3: 1589.0000 - false_negatives_3: 1043.0000\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_01_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 239s 1s/step - loss: 0.6806 - acc: 0.5619 - true_positives_3: 1961.0000 - true_negatives_3: 1415.0000 - false_positives_3: 1589.0000 - false_negatives_3: 1043.0000 - val_loss: 0.6291 - val_acc: 0.9797 - val_true_positives_3: 3139.0000 - val_true_negatives_3: 3.0000 - val_false_positives_3: 47.0000 - val_false_negatives_3: 18.0000\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6457 - acc: 0.6388 - true_positives_3: 1730.0000 - true_negatives_3: 2108.0000 - false_positives_3: 896.0000 - false_negatives_3: 1274.0000\n",
      "Epoch 2: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_02_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 234s 1s/step - loss: 0.6457 - acc: 0.6388 - true_positives_3: 1730.0000 - true_negatives_3: 2108.0000 - false_positives_3: 896.0000 - false_negatives_3: 1274.0000 - val_loss: 0.6212 - val_acc: 0.5856 - val_true_positives_3: 1845.0000 - val_true_negatives_3: 33.0000 - val_false_positives_3: 17.0000 - val_false_negatives_3: 1312.0000\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6154 - acc: 0.6871 - true_positives_3: 1715.0000 - true_negatives_3: 2413.0000 - false_positives_3: 591.0000 - false_negatives_3: 1289.0000\n",
      "Epoch 3: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_03_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 234s 1s/step - loss: 0.6154 - acc: 0.6871 - true_positives_3: 1715.0000 - true_negatives_3: 2413.0000 - false_positives_3: 591.0000 - false_negatives_3: 1289.0000 - val_loss: 0.5983 - val_acc: 0.6327 - val_true_positives_3: 1997.0000 - val_true_negatives_3: 32.0000 - val_false_positives_3: 18.0000 - val_false_negatives_3: 1160.0000\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5976 - acc: 0.6987 - true_positives_3: 1830.0000 - true_negatives_3: 2368.0000 - false_positives_3: 636.0000 - false_negatives_3: 1174.0000\n",
      "Epoch 4: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_04_Clasificacion_clase_20220601_161631.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 294s 2s/step - loss: 0.5976 - acc: 0.6987 - true_positives_3: 1830.0000 - true_negatives_3: 2368.0000 - false_positives_3: 636.0000 - false_negatives_3: 1174.0000 - val_loss: 0.6146 - val_acc: 0.5853 - val_true_positives_3: 1844.0000 - val_true_negatives_3: 33.0000 - val_false_positives_3: 17.0000 - val_false_negatives_3: 1313.0000\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5864 - acc: 0.7086 - true_positives_3: 1913.0000 - true_negatives_3: 2344.0000 - false_positives_3: 660.0000 - false_negatives_3: 1091.0000\n",
      "Epoch 5: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_05_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 325s 2s/step - loss: 0.5864 - acc: 0.7086 - true_positives_3: 1913.0000 - true_negatives_3: 2344.0000 - false_positives_3: 660.0000 - false_negatives_3: 1091.0000 - val_loss: 0.6081 - val_acc: 0.6405 - val_true_positives_3: 2022.0000 - val_true_negatives_3: 32.0000 - val_false_positives_3: 18.0000 - val_false_negatives_3: 1135.0000\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5826 - acc: 0.7125 - true_positives_3: 1992.0000 - true_negatives_3: 2289.0000 - false_positives_3: 715.0000 - false_negatives_3: 1012.0000\n",
      "Epoch 6: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_06_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 323s 2s/step - loss: 0.5826 - acc: 0.7125 - true_positives_3: 1992.0000 - true_negatives_3: 2289.0000 - false_positives_3: 715.0000 - false_negatives_3: 1012.0000 - val_loss: 0.6040 - val_acc: 0.6408 - val_true_positives_3: 2023.0000 - val_true_negatives_3: 32.0000 - val_false_positives_3: 18.0000 - val_false_negatives_3: 1134.0000\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5776 - acc: 0.7172 - true_positives_3: 2058.0000 - true_negatives_3: 2251.0000 - false_positives_3: 753.0000 - false_negatives_3: 946.0000\n",
      "Epoch 7: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_07_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 238s 1s/step - loss: 0.5776 - acc: 0.7172 - true_positives_3: 2058.0000 - true_negatives_3: 2251.0000 - false_positives_3: 753.0000 - false_negatives_3: 946.0000 - val_loss: 0.5918 - val_acc: 0.7010 - val_true_positives_3: 2217.0000 - val_true_negatives_3: 31.0000 - val_false_positives_3: 19.0000 - val_false_negatives_3: 940.0000\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5744 - acc: 0.7204 - true_positives_3: 2082.0000 - true_negatives_3: 2246.0000 - false_positives_3: 758.0000 - false_negatives_3: 922.0000\n",
      "Epoch 8: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_08_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 241s 1s/step - loss: 0.5744 - acc: 0.7204 - true_positives_3: 2082.0000 - true_negatives_3: 2246.0000 - false_positives_3: 758.0000 - false_negatives_3: 922.0000 - val_loss: 0.5799 - val_acc: 0.7007 - val_true_positives_3: 2216.0000 - val_true_negatives_3: 31.0000 - val_false_positives_3: 19.0000 - val_false_negatives_3: 941.0000\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5718 - acc: 0.7215 - true_positives_3: 2117.0000 - true_negatives_3: 2218.0000 - false_positives_3: 786.0000 - false_negatives_3: 887.0000\n",
      "Epoch 9: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_09_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 239s 1s/step - loss: 0.5718 - acc: 0.7215 - true_positives_3: 2117.0000 - true_negatives_3: 2218.0000 - false_positives_3: 786.0000 - false_negatives_3: 887.0000 - val_loss: 0.5803 - val_acc: 0.7007 - val_true_positives_3: 2216.0000 - val_true_negatives_3: 31.0000 - val_false_positives_3: 19.0000 - val_false_negatives_3: 941.0000\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5704 - acc: 0.7214 - true_positives_3: 2128.0000 - true_negatives_3: 2206.0000 - false_positives_3: 798.0000 - false_negatives_3: 876.0000\n",
      "Epoch 10: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_10_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 268s 1s/step - loss: 0.5704 - acc: 0.7214 - true_positives_3: 2128.0000 - true_negatives_3: 2206.0000 - false_positives_3: 798.0000 - false_negatives_3: 876.0000 - val_loss: 0.5748 - val_acc: 0.7044 - val_true_positives_3: 2230.0000 - val_true_negatives_3: 29.0000 - val_false_positives_3: 21.0000 - val_false_negatives_3: 927.0000\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5694 - acc: 0.7212 - true_positives_3: 2133.0000 - true_negatives_3: 2200.0000 - false_positives_3: 804.0000 - false_negatives_3: 871.0000\n",
      "Epoch 11: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_11_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 322s 2s/step - loss: 0.5694 - acc: 0.7212 - true_positives_3: 2133.0000 - true_negatives_3: 2200.0000 - false_positives_3: 804.0000 - false_negatives_3: 871.0000 - val_loss: 0.5750 - val_acc: 0.7044 - val_true_positives_3: 2230.0000 - val_true_negatives_3: 29.0000 - val_false_positives_3: 21.0000 - val_false_negatives_3: 927.0000\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5709 - acc: 0.7214 - true_positives_3: 2127.0000 - true_negatives_3: 2207.0000 - false_positives_3: 797.0000 - false_negatives_3: 877.0000\n",
      "Epoch 12: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_12_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 234s 1s/step - loss: 0.5709 - acc: 0.7214 - true_positives_3: 2127.0000 - true_negatives_3: 2207.0000 - false_positives_3: 797.0000 - false_negatives_3: 877.0000 - val_loss: 0.5753 - val_acc: 0.7028 - val_true_positives_3: 2225.0000 - val_true_negatives_3: 29.0000 - val_false_positives_3: 21.0000 - val_false_negatives_3: 932.0000\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5698 - acc: 0.7209 - true_positives_3: 2149.0000 - true_negatives_3: 2182.0000 - false_positives_3: 822.0000 - false_negatives_3: 855.0000\n",
      "Epoch 13: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_13_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 236s 1s/step - loss: 0.5698 - acc: 0.7209 - true_positives_3: 2149.0000 - true_negatives_3: 2182.0000 - false_positives_3: 822.0000 - false_negatives_3: 855.0000 - val_loss: 0.5825 - val_acc: 0.7025 - val_true_positives_3: 2224.0000 - val_true_negatives_3: 29.0000 - val_false_positives_3: 21.0000 - val_false_negatives_3: 933.0000\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5686 - acc: 0.7212 - true_positives_3: 2150.0000 - true_negatives_3: 2183.0000 - false_positives_3: 821.0000 - false_negatives_3: 854.0000\n",
      "Epoch 14: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_14_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 236s 1s/step - loss: 0.5686 - acc: 0.7212 - true_positives_3: 2150.0000 - true_negatives_3: 2183.0000 - false_positives_3: 821.0000 - false_negatives_3: 854.0000 - val_loss: 0.5824 - val_acc: 0.7031 - val_true_positives_3: 2226.0000 - val_true_negatives_3: 29.0000 - val_false_positives_3: 21.0000 - val_false_negatives_3: 931.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5678 - acc: 0.7204 - true_positives_3: 2157.0000 - true_negatives_3: 2171.0000 - false_positives_3: 833.0000 - false_negatives_3: 847.0000\n",
      "Epoch 15: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_DATO\\CLASE_TrainDS_1\\Model_15_Clasificacion_clase_20220601_161631.hdf5\n",
      "188/188 [==============================] - 230s 1s/step - loss: 0.5678 - acc: 0.7204 - true_positives_3: 2157.0000 - true_negatives_3: 2171.0000 - false_positives_3: 833.0000 - false_negatives_3: 847.0000 - val_loss: 0.6010 - val_acc: 0.7022 - val_true_positives_3: 2223.0000 - val_true_negatives_3: 29.0000 - val_false_positives_3: 21.0000 - val_false_negatives_3: 934.0000\n",
      "Epoch 16/20\n",
      "125/188 [==================>...........] - ETA: 52s - loss: 0.5741 - acc: 0.7172 - true_positives_3: 1457.0000 - true_negatives_3: 1412.0000 - false_positives_3: 575.0000 - false_negatives_3: 556.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultados = trainModel(p_train,path_imagenes,path_base,products,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328e353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
