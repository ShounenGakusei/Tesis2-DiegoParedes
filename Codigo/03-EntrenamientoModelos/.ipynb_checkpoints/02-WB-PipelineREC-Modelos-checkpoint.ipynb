{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b716303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \n",
    "\"\"\"\n",
    "Autor='Diego Paredes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443f8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53969cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/FinalTesis/Tesis2-DiegoParedes'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH DEL PROYECTO \n",
    "\"\"\"\n",
    "with open('../../path_base.txt') as f:\n",
    "    path_base = f.read()\n",
    "path_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables generales\n",
    "\"\"\"\n",
    "path_imagenes = 'F:/GOES/'     \n",
    "\n",
    "products = ['C13','C07','C08']\n",
    "times   = ['10','20','30','40','50','00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#Limitamos el GPU, en caso se necesite\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c44d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConv3D_func(p,run,input_1):   \n",
    "    \n",
    "    x = input_1\n",
    "    if p['normLayer'][run]:\n",
    "        x = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "        \n",
    "    x = tf.keras.layers.Conv3D(32, (3,3,3), input_shape=(shape),padding='same', activation='relu')(x)    \n",
    "    \n",
    "    \n",
    "    for iConv in range(p['cnn_cant'][run]):\n",
    "        units = p['cnn_units'][run][iConv]\n",
    "        maxPool = p['maxPool'][run][iConv]\n",
    "        droupout = p['droupout'][run][iConv]        \n",
    "        \n",
    "        x = tf.keras.layers.Conv3D(units, (3,3,3), padding='same', activation='relu')(x)\n",
    "        if maxPool:\n",
    "            x = tf.keras.layers.MaxPooling3D()(x)\n",
    "        if droupout:\n",
    "            x = tf.keras.layers.Dropout(droupout)(x)\n",
    "   \n",
    "    \n",
    "    output = tf.keras.layers.GlobalMaxPool3D()(x)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConv2D_func(p,run,input_1):   \n",
    "    \n",
    "    x = input_1\n",
    "    if p['normLayer'][run]:\n",
    "        x = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(32, 3, input_shape=(shape),padding='same', activation='relu')(x)    \n",
    "    \n",
    "    \n",
    "    for iConv in range(p['cnn_cant'][run]):\n",
    "        units = p['cnn_units'][run][iConv]\n",
    "        maxPool = p['maxPool'][run][iConv]\n",
    "        droupout = p['droupout'][run][iConv]        \n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(units, 3, padding='same', activation='relu')(x)\n",
    "        if maxPool:\n",
    "            x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        if droupout:\n",
    "            x = tf.keras.layers.Dropout(droupout)(x)\n",
    "   \n",
    "    \n",
    "    output = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f5f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConv2D(p,run):    \n",
    "    model = keras.Sequential()\n",
    "    shape = (p['margen'][run],p['margen'][run],p['canales'][run])\n",
    "        \n",
    "    if p['normLayer'][run]:\n",
    "        model.add(tf.keras.layers.Rescaling(1./65536))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3), input_shape=(p['margen'][run],p['margen'][run],p['canales'][run]),padding='same', activation='relu'))    \n",
    "    \n",
    "    for iConv in range(p['cnn_cant'][run]):\n",
    "        units = p['cnn_units'][run][iConv]\n",
    "        maxPool = p['maxPool'][run][iConv]\n",
    "        droupout = p['droupout'][run][iConv]        \n",
    "        \n",
    "        model.add(tf.keras.layers.Conv2D(units, (3,3), padding='same', activation='relu'))\n",
    "        if maxPool:\n",
    "            model.add(tf.keras.layers.MaxPooling2D())\n",
    "        if droupout:\n",
    "            model.add(tf.keras.layers.Dropout(droupout))       \n",
    "   \n",
    "    \n",
    "    model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(p,run, redTipo):    \n",
    "    \n",
    "    shape = (p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run])\n",
    "    \n",
    "    \n",
    "    if p['rnn_tipo'][run] == 'LSTM':\n",
    "        input_1 = tf.keras.layers.Input(shape=shape)\n",
    "        convnet = getConv2D(p,run)\n",
    "        timeD = tf.keras.layers.TimeDistributed(convnet)(input_1)\n",
    "        _rnn =  tf.keras.layers.LSTM(P['rnn_units'])(timeD)\n",
    "        listConcat = [_rnn]\n",
    "    elif p['rnn_tipo'][run] == 'CONV3D':\n",
    "        input_1 = tf.keras.layers.Input(shape=shape)\n",
    "        convnet = getConv3D_func(p,run,input_1)    \n",
    "        listConcat = [convnet]        \n",
    "    elif p['rnn_tipo'][run] == 'CONV2D':\n",
    "        input_1 = tf.keras.layers.Input(shape=shape[1:])\n",
    "        convnet = getConv2D_func(p,run,input_1)    \n",
    "        listConcat = [convnet]\n",
    "    else:\n",
    "        print(f\"ERROR: No se especifico un tipo de red correcto... {p['rnn_tipo'] }\")\n",
    "        \n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'][run])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][run][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat) \n",
    "    \n",
    "    \n",
    "    \n",
    "    dense_capas = [final]\n",
    "    for iDense in range(p['dense_cant'][run]):\n",
    "        units = p['dense_units'][run][iDense]\n",
    "        dense_capas.append(tf.keras.layers.Dense(units=units, activation=tf.keras.activations.relu)(dense_capas[iDense]))\n",
    "               \n",
    "    \n",
    "    # output\n",
    "    if redTipo == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_capas[-1])      \n",
    "    elif redTipo == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_capas[-1])\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(params, HP, run):\n",
    "    redTipo = params['redTipo']\n",
    "    paciencia = params['paciencia']\n",
    "    \n",
    "    lr = HP['lr'][run]    \n",
    "    \n",
    "    if redTipo == 'Clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)   \n",
    "        if HP['loss'][run] == 'binary_crossentropy':\n",
    "            loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        \n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif redTipo == 'Regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(img, DA):\n",
    "    if DA == 1:\n",
    "        return tf.image.flip_left_right(img)\n",
    "    elif DA == 2:\n",
    "        return tf.image.flip_up_down(img)\n",
    "    elif DA == 3:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        return tf.image.flip_up_down(img)   \n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value,p, run, path_base, DA=0, _3D=False, redTipo='Clasificacion'):\n",
    "    # imagenData[0] = XO\n",
    "    # imagenData[1] = XA\n",
    "    # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    \n",
    "    \n",
    "    size = int(p['margen'][run] / 2)\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "        \n",
    "        if DA:\n",
    "            img_decoded = applyDA(img_decoded, item['DA'])\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if _3D:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs'][run]) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs'][run]:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    if redTipo=='Regresion':\n",
    "        return tuple(itemL), float(value)\n",
    "    else:     \n",
    "        return tuple(itemL), int(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p,HP,run, path_imagenes):\n",
    "    \n",
    "    test = pd.read_csv(p['dsVal'])\n",
    "    train = pd.read_csv(p['dsTrain'])\n",
    "    \n",
    "    if p['dataset']:\n",
    "        train =  train.sample(frac=p['dataset'])\n",
    "        test = test.sample(frac=p['dataset'])\n",
    "    \n",
    "               \n",
    "    inputsList = {}\n",
    "    inputsListTest = {}\n",
    "    \n",
    "    # Agregamos un atributo para indicar que el dato va realizar DA\n",
    "    if p['DA']:        \n",
    "        inputsList['DA'] = train['DA'].tolist() \n",
    "        \n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}') \n",
    "    \n",
    "    for inp in HP['inputs'][run]:\n",
    "        inputsList[inp] = train[inp].tolist()  \n",
    "        inputsListTest[inp] = test[inp].tolist()  \n",
    "        \n",
    "       \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[HP['outputs'][run]].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsListTest),test[HP['outputs'][run]].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,HP,run,path_imagenes,p['DA'],p['meanMatrizImagen'],p['redTipo']))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,HP,run,path_imagenes,False,p['meanMatrizImagen'],p['redTipo']))\n",
    "       \n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])\n",
    "    val_dataset = val_dataset.batch(p['batch'])\n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5adbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDir(path, newDir):\n",
    "    try:\n",
    "        pathT = os.path.join(path, newDir)\n",
    "        os.mkdir(pathT)\n",
    "        return pathT\n",
    "    except FileExistsError:\n",
    "        return pathT\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear el directorio: {newDir}\")\n",
    "        pritn(f'Path base: {path}')\n",
    "        pritn(f'Nuevo    : {newDir}')        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67dc36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iniciarProyect(path_base, params, HP):\n",
    "    repDir = crearDir(f'{path_base}/Archivos/Resultados', params[\"redTipo\"])   \n",
    "    repDir = crearDir(repDir, params[\"directory\"])\n",
    "    repDir = crearDir(repDir, params[\"Proyect\"])\n",
    "    \n",
    "    excelFile = f'{repDir}/Stats-{params[\"Proyect\"]}.xlsx'\n",
    "    \n",
    "    if params['record'] and not os.path.exists(excelFile):\n",
    "        writer = pd.ExcelWriter(excelFile, engine = 'xlsxwriter')\n",
    "        #joinParams = params | HP\n",
    "        keys_values = params.items()\n",
    "        strParams = {str(key): str(value) for key, value in keys_values}        \n",
    "        \n",
    "        pd.DataFrame(strParams,index=[0]).to_excel(writer, sheet_name = 'Informacion')\n",
    "        \n",
    "        pd.DataFrame(HP).to_excel(writer, sheet_name = 'Informacion',startrow=3)\n",
    "        \n",
    "        writer.save()     \n",
    "    return repDir , excelFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03005a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCM(logs):\n",
    "    lKeys = list(logs.keys())\n",
    "    \n",
    "    try:\n",
    "        TN = int(logs[[x for x in lKeys if 'val_true_negatives' in x][0]])\n",
    "        TP = int(logs[[x for x in lKeys if 'val_true_positives' in x][0]])\n",
    "        FN = int(logs[[x for x in lKeys if 'val_false_negatives' in x][0]])\n",
    "        FP = int(logs[[x for x in lKeys if 'val_false_positives' in x][0]])\n",
    "    except:\n",
    "        print(f'\\nNo se pudo leer keys para la matriz de confucion en logs : {lKeys}')\n",
    "        print(f'Se intento leer: val_true_negatives,val_true_positives, val_false_negatives y val_false_positives')\n",
    "    \n",
    "       \n",
    "    y_true =  [0]*TN + [1]*TP + [1]*FN + [0]*FP\n",
    "    _y_pred = [0]*TN + [1]*TP + [0]*FN + [1]*FP\n",
    "    \n",
    "    return TN, FP, FN, TP, np.array(y_true), np.array(_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d2f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCB(Callback):\n",
    "    \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "    def __init__(self, val_ds=None, WANDB=True):     \n",
    "        self.val_ds = val_ds  \n",
    "        self.history = {}\n",
    "        self.wandb = WANDB\n",
    "   \n",
    "   \n",
    "    def on_epoch_end(self, epoch, logs={}):  \n",
    "        TN, FP, FN, TP, y_true, _y_pred = getCM(logs)\n",
    "        \n",
    "        self.history.setdefault('epoch', []).append(epoch)\n",
    "        \n",
    "        self.history.setdefault('loss', []).append(logs['loss']) \n",
    "        self.history.setdefault('acc', []).append(logs['acc'])  \n",
    "        self.history.setdefault('val_loss', []).append(logs['val_loss']) \n",
    "        self.history.setdefault('val_acc', []).append(logs['val_acc']) \n",
    "        \n",
    "        \n",
    "        self.history.setdefault('val_TN', []).append(TN) \n",
    "        self.history.setdefault('val_FP', []).append(FP)\n",
    "        self.history.setdefault('val_FN', []).append(FN) \n",
    "        self.history.setdefault('val_TP', []).append(TP) \n",
    "        \n",
    "            \n",
    "        if self.wandb:\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                    preds=_y_pred, y_true=y_true,\n",
    "                                    class_names=[0,1]),                   \n",
    "                       'val_TN' :TN,'val_FN' :FN,'val_TP' :TP,'val_FP' :FP,\n",
    "                       'val_acc': logs['val_acc'],'loss' : logs['loss'],\n",
    "                       'val_loss': logs['val_loss'],'acc' : logs['acc']                  \n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1c5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearCallbacks(statsDir, HP,run, metricas, p):\n",
    "    CB = metricas['callbacks']\n",
    "    \n",
    "    idModel = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = statsDir + '/Model_{epoch:02d}_' + f'{HP[\"rnn_tipo\"][run]}_{HP[\"outputs\"][run]}_{idModel}.hdf5' \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,verbose=1)\n",
    "    \n",
    "    # Iniciamos WANDB\n",
    "    if p['record']:        \n",
    "        CB.append(cp_callback)   \n",
    "     \n",
    "    if p['WANDB']:\n",
    "        config = dict(learning_rate=HP['lr'][run], epochs = p['epocas'],\n",
    "             batch_size = p['batch'],architecture=\"CNN\", \n",
    "             num_classes = 2,)\n",
    "        wandb.init(project=f'{p[\"Proyect\"]}',            \n",
    "                   config=config,\n",
    "                   name= f'Ex_({HP[\"canales\"][run]}-{HP[\"tiempos\"][run]}-{HP[\"margen\"][run]})_{idModel}')   \n",
    "                                               \n",
    "    return CB, idModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(path_base,path_imagenes,params,HP, criterio):        \n",
    "    \"\"\" Creamos los directorios para los reportes  \"\"\"             \n",
    "    repDir, statsFile  = iniciarProyect(path_base, params, HP)\n",
    "    print(f'DIRECTORIO BASE : {repDir}')\n",
    "    \n",
    "    \n",
    "    \"\"\" Comenzamos el entrenamiento \"\"\" \n",
    "    # Una iteracion por cada Hiperparametro (HP) que existe\n",
    "    ds_i = 0  \n",
    "    resultados = [] \n",
    "    for run in range(HP['runs']):\n",
    "        ds_i += 1                 \n",
    "        print(f'Inicio de la prueba N°: {(run+1)}/{HP[\"runs\"]}')         \n",
    "        print(f'- Nombre del Proyecto : {params[\"Proyect\"]}')\n",
    "        print(f'- Batch size          : {params[\"batch\"]}')\n",
    "        print(f'- Criterio {criterio} : {HP[criterio][run]}')\n",
    "        print('__________________________________________________')        \n",
    "        \n",
    "        \"\"\" LEEMOS EL DATASET A USAR  \"\"\"     \n",
    "        train_dataset, val_dataset = splitDataset(params,HP,run, path_imagenes)\n",
    "        \n",
    "\n",
    "        \"\"\" DEFINIMOS Y INICAMOS EL MODELO \"\"\"\n",
    "        model = crearModelo(HP,run,params['redTipo']) \n",
    "        metricas = getMetrics(params, HP, run)\n",
    "        model.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "        \n",
    "\n",
    "        \"\"\" CALLBACKS \"\"\"\n",
    "        CB, idModel = crearCallbacks(repDir, HP,run, metricas, params)           \n",
    "        if params['redTipo'] == 'Clasificacion':\n",
    "            hist =  CustomCB(val_dataset, params['WANDB'])\n",
    "\n",
    "            \n",
    "        \"\"\" ENTRENAMIENTO \"\"\"\n",
    "        history = model.fit(train_dataset,batch_size=params['batch'],                            \n",
    "                            epochs=params['epocas'],callbacks=CB,\n",
    "                            validation_data=val_dataset,\n",
    "                            validation_batch_size=params['batch'],\n",
    "                            verbose=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\" GUARDAMOS REPORTES \"\"\"                \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            resultados.append(history.history)\n",
    "            # Guardamos las estadisticas\n",
    "            if params['record']:                \n",
    "                with pd.ExcelWriter(statsFile, mode=\"a\", engine=\"openpyxl\", if_sheet_exists='overlay') as writer:                    \n",
    "                    tempDF = pd.DataFrame(history.history)\n",
    "                    if params['redTipo'] == 'Clasificacion':\n",
    "                        tempDF.columns = ['loss', 'acc', 'TP', 'TN', 'FP','FN',\n",
    "                                          'val_loss','val_acc','val_TP','val_TN','val_FP','val_FN']\n",
    "                    tempDF.to_excel(writer,startrow=0,\n",
    "                                    sheet_name=f'{run}-{idModel}')\n",
    "            if params['WANDB']:\n",
    "                wandb.finish()            \n",
    "        except:\n",
    "            print(f'ERROR: No se pudo guardar los resulatdos test: {run}')\n",
    "            try:\n",
    "                with open(f'{repDir}/Test-{run}-{idModel}.csv', 'wb') as file_pi:\n",
    "                    pickle.dump(history.history, file_pi)\n",
    "            except Exception:\n",
    "                print(traceback.format_exc())\n",
    "                print(f'ERROR: No se pudo guardar el hist temporal')\n",
    "                return history\n",
    "            \n",
    "        \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9ce9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#from wandb.keras import WandbCallback\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56ff0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALDIACION : 3207\n",
      "--------------------------------------------------\n",
      "TRAIN: 6008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>...</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>umb1</th>\n",
       "      <th>umb2</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "      <th>DA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38368</td>\n",
       "      <td>SANTA CLOTILDE</td>\n",
       "      <td>47271776</td>\n",
       "      <td>548</td>\n",
       "      <td>215</td>\n",
       "      <td>-73.67925</td>\n",
       "      <td>-2.48757</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>9.936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2021-10-04-08</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>D01</td>\n",
       "      <td>548--215--2021-10-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38369</td>\n",
       "      <td>INTIHUATANA H</td>\n",
       "      <td>472935F2</td>\n",
       "      <td>610</td>\n",
       "      <td>808</td>\n",
       "      <td>-72.56342</td>\n",
       "      <td>-13.17408</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2021-12-24-13</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>610--808--2021-12-24-13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0          nombre    codigo   XO   XA  longitud  \\\n",
       "0             0       38368  SANTA CLOTILDE  47271776  548  215 -73.67925   \n",
       "1             1       38369   INTIHUATANA H  472935F2  610  808 -72.56342   \n",
       "\n",
       "    latitud  altura  dato  ...    99%  75%  umb1  umb2          fecha  \\\n",
       "0  -2.48757   150.0   0.3  ...  9.936  0.0   0.6   6.6  2021-10-04-08   \n",
       "1 -13.17408  1778.0   0.2  ...  0.000  0.0   3.1   7.5  2021-12-24-13   \n",
       "\n",
       "       flag flagV2                   imagen clase  DA  \n",
       "0  M0000002    D01  548--215--2021-10-04-08     0   3  \n",
       "1  M0000002    C01  610--808--2021-12-24-13     0   3  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Definimos las varibles para las iteraciones\n",
    "\"\"\"\n",
    "\n",
    "modelTipo = 'Clasificacion'\n",
    "rnnTipo = 'LSTM' #'LSTM'\n",
    "idProject = datetime.today().strftime(\"%Y%m%d_%H\")\n",
    "\n",
    "p_train = {\n",
    "            # Variables generales\n",
    "          'products': products,\n",
    "          'times'   : times,\n",
    "    \n",
    "            # Reportes\n",
    "          'directory': 'RNN', # 'RNN'\n",
    "          'Proyect'  : f'{modelTipo}-{idProject}', # TesisDiego\n",
    "          'record'   : True,  # Grabar los resultados en  excels    \n",
    "          'WANDB'    : False, # Grabar los resultados en WANDB\n",
    "    \n",
    "            # Datos del modelo\n",
    "          'redTipo'  : modelTipo, # Clasificacion / Regresion\n",
    "          'rnn'      : True,  # Redes recurrentes          \n",
    "          'meanMatrizImagen' : False, # !!! RNN modelos SIEMPRE EN FALSE  !!!\n",
    "          \n",
    "        \n",
    "            # Variables del entrenamiento                \n",
    "          'batch'    : 32,     \n",
    "          'epocas'   : 40,  \n",
    "          'paciencia': 15,   # 0 = No paciencia  \n",
    "    \n",
    "    \n",
    "           # Dataset\n",
    "          'dsTrain'  : f'{path_base}/Archivos/Dataset/{modelTipo}/Entrenamiento/SplitConDA_20C_DM_V2/CLASE_TrainDS_1.csv', \n",
    "          'dsVal'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Validacion/ClaseV2_DUD_ValidacionDS.csv',\n",
    "          'dataset'  : 1,     # 1 = 100% del ds\n",
    "          'DA'       : True,  # Usaulmente para clasificacion          \n",
    "         }\n",
    "\n",
    "cantRuns = 4\n",
    "\n",
    " # Hiper parametros     \n",
    "hiperparams = {    \n",
    "               # General\n",
    "              'dsTName'     :['SplitConDA_20C_DM_V2']*cantRuns,               \n",
    "              'inputs'     : [['imagen', 'dato','altura'],['imagen', 'dato','altura'],\n",
    "                              ['imagen', 'dato','umb1','altura'],['imagen', 'dato','umb1','altura']],\n",
    "              'outputs'    : ['clase']*cantRuns,\n",
    "    \n",
    "              # Modelo\n",
    "              'lr'         : [0.001]*cantRuns, \n",
    "              'loss'       : ['binary_crossentropy']*cantRuns,\n",
    "              'normLayer'  : [True]*cantRuns,\n",
    "    \n",
    "               # Capas convulucionales\n",
    "              'cnn_cant'   : [1]*cantRuns,\n",
    "              'cnn_units'  : [[128],[32],[16],[8]],\n",
    "              'droupout'   : [[0.2]]*cantRuns,\n",
    "              'maxPool'    : [[False]]*cantRuns,              \n",
    "    \n",
    "               # Capas Recurrentes\n",
    "              'rnn_tipo'   : [rnnTipo]*cantRuns,\n",
    "              'rnn_units'  : [32]*cantRuns,    \n",
    "              \n",
    "              # Capas densas\n",
    "              'dense_cant' : [4]*cantRuns,\n",
    "              'dense_units': [[64,32,32,16]]*cantRuns,\n",
    "              \n",
    "              # Imagenes satelitates\n",
    "              'canales'    : [3]*cantRuns,\n",
    "              'tiempos'    : [6]*cantRuns,\n",
    "              'margen'     : [14]*cantRuns,\n",
    "    \n",
    "              # -\n",
    "              'runs'       : cantRuns\n",
    "              }\n",
    "\n",
    "\n",
    "# Forma del DS\n",
    "if p_train['dsVal']:\n",
    "    dfVal = pd.read_csv(p_train['dsVal'])\n",
    "    print(f'VALDIACION : {len(dfVal)}')\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "tempDF = pd.read_csv(p_train['dsTrain'])\n",
    "print(f'TRAIN: {len(tempDF)}')\n",
    "tempDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62626c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORIO BASE : C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/FinalTesis/Tesis2-DiegoParedes/Archivos/Resultados\\Clasificacion\\RNN\\Clasificacion-20230105_18\n",
      "Inicio de la prueba N°: 1/4\n",
      "- Nombre del Proyecto : Clasificacion-20230105_18\n",
      "- Batch size          : 32\n",
      "- Criterio margen : 14\n",
      "__________________________________________________\n",
      "Tamaño del dataset: Train 6008  - Val 3207\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'units_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(path_base, path_imagenes, params, HP, criterio)\u001b[0m\n\u001b[0;32m     20\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m splitDataset(params,HP,run, path_imagenes)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\" DEFINIMOS Y INICAMOS EL MODELO \"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcrearModelo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHP\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mredTipo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     25\u001b[0m metricas \u001b[38;5;241m=\u001b[39m getMetrics(params, HP, run)\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mmetricas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m],loss\u001b[38;5;241m=\u001b[39mmetricas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_fn\u001b[39m\u001b[38;5;124m'\u001b[39m],metrics\u001b[38;5;241m=\u001b[39mmetricas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m],)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mcrearModelo\u001b[1;34m(p, run, redTipo)\u001b[0m\n\u001b[0;32m      8\u001b[0m     convnet \u001b[38;5;241m=\u001b[39m getConv2D(p,run)\n\u001b[0;32m      9\u001b[0m     timeD \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTimeDistributed(convnet)(input_1)\n\u001b[1;32m---> 10\u001b[0m     _rnn \u001b[38;5;241m=\u001b[39m  tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLSTM(\u001b[43munits_rnn\u001b[49m)(timeD)\n\u001b[0;32m     11\u001b[0m     listConcat \u001b[38;5;241m=\u001b[39m [_rnn]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_tipo\u001b[39m\u001b[38;5;124m'\u001b[39m][run] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCONV3D\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'units_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultado = trainModel(path_base,path_imagenes,p_train,hiperparams, 'margen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ce07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e2dff-8ad3-4d31-b4e8-a87c803d7199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
