{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b716303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \n",
    "\"\"\"\n",
    "Autor='Diego Paredes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "443f8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53969cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH DEL PROYECTO \n",
    "\"\"\"\n",
    "with open('../../path_base.txt') as f:\n",
    "    path_base = f.read()\n",
    "path_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca3336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables generales\n",
    "\"\"\"\n",
    "path_imagenes = 'F:/GOES/'     \n",
    "\n",
    "products = ['C13','C07','C08']\n",
    "times   = ['10','20','30','40','50','00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#Limitamos el GPU, en caso se necesite\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1715a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo2D(p,run):    \n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    mxPool_1 = tf.keras.layers.MaxPooling2D()(conv2d_1)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(mxPool_1)\n",
    "    \n",
    "    conv2d_2 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling2D()(conv2d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)\n",
    "    \n",
    "    #conv2d_3 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    #mxPool_3 = tf.keras.layers.MaxPooling2D()(conv2d_3)\n",
    "    #dropout_3  = tf.keras.layers.Dropout(0.2)(mxPool_3)\n",
    "    \n",
    "    #conv2d_4 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_3)\n",
    "    #mxPool_4 = tf.keras.layers.MaxPooling2D()(conv2d_4)\n",
    "    #dropout_4  = tf.keras.layers.Dropout(0.2)(mxPool_4)\n",
    "    \n",
    "    conv2d_5 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv2d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2589bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo3D(p,run):        \n",
    "    # Imagen                                 (6, 30, 30, 3)\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    \n",
    "    conv3d_1 = tf.keras.layers.Conv3D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(conv3d_1)\n",
    "    \n",
    "    conv3d_2 = tf.keras.layers.Conv3D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling3D()(conv3d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)    \n",
    "\n",
    "    \n",
    "    conv3d_5 = tf.keras.layers.Conv3D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv3d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(modelType, lr, paciencia):\n",
    "    \n",
    "    if modelType == 'Clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr) \n",
    "        \n",
    "        #BinaryCrossentropy() #CategoricalCrossentropy()      \n",
    "        loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif modelType == 'Regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(params,run):\n",
    "    \n",
    "    if params['meanMatrizImagen']:\n",
    "        print(f\"Creando modelo 2D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo2D(params,run)\n",
    "    else:\n",
    "        print(f\"Creando modelo 3D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo3D(params,run)\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "926f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(img, DA):\n",
    "    if DA == 1:\n",
    "        return tf.image.flip_left_right(img)\n",
    "    elif DA == 2:\n",
    "        return tf.image.flip_up_down(img)\n",
    "    elif DA == 3:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        return tf.image.flip_up_down(img)   \n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value, p,run, path_base, products, times, DA=0):\n",
    "    # imagenData[0] = XO     # imagenData[1] = XA     # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    size = int(p['margen'][run] / 2)\n",
    "\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "        \n",
    "        if DA:\n",
    "            img_decoded = applyDA(img_decoded, item['DA'])\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if p['meanMatrizImagen']:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs']) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs']:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    if p['redTipo']=='Regresion':\n",
    "        print('es regresion')\n",
    "        return tuple(itemL), float(value)\n",
    "    else:\n",
    "        return tuple(itemL), int(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p, run, dataset, path_imagenes, products, times,val_split= 0.2):\n",
    "            \n",
    "    if p['dsVal']:\n",
    "        test = pd.read_csv(p['dsVal'])\n",
    "        if p['dataset']:\n",
    "            train =  dataset.sample(frac=p['dataset'])\n",
    "            test = test.sample(frac=p['dataset'])\n",
    "        else:\n",
    "            train =  dataset\n",
    "            \n",
    "    else:\n",
    "        # Escojemos una fraccion del dataset si se ha indicado           \n",
    "        if p['dataset']:\n",
    "            train, test = train_test_split(dataset.sample(frac=p['dataset']), test_size=val_split, shuffle=True)\n",
    "        else:\n",
    "            train, test = train_test_split(dataset, test_size=val_split, shuffle=True)\n",
    "               \n",
    "    inputsList = {}\n",
    "    inputsListTest = {}\n",
    "    # Agregamos un atributo para indicar que el dato va realizar DA\n",
    "    if p['DA']:        \n",
    "        inputsList['DA'] = train['DA'].tolist() \n",
    "        #inputsListTest['DA'] = test['DA'].tolist() \n",
    "        \n",
    "        \n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}')    \n",
    "    \n",
    "    for inp in p['inputs']:\n",
    "        inputsList[inp] = train[inp].tolist()  \n",
    "        inputsListTest[inp] = test[inp].tolist()  \n",
    "        \n",
    "       \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsListTest),test[p['outputs']].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,p['DA']))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times,False))#p['DA']\n",
    "       \n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])#.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(p['batch'])#.prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5adbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDir(path, newDir):\n",
    "    try:\n",
    "        pathT = os.path.join(path, newDir)\n",
    "        os.mkdir(pathT)\n",
    "        return pathT\n",
    "    except FileExistsError:\n",
    "        return pathT\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear el directorio: {newDir}\")\n",
    "        pritn(f'Path base: {path}')\n",
    "        pritn(f'Nuevo    : {newDir}')        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c195161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializarVariables(repDir, params,ds):\n",
    "    # Leemos dataset\n",
    "    try:\n",
    "        dataset = pd.read_csv(ds)        \n",
    "        dsName = ds.split('/')[-1][:-4]\n",
    "    except:\n",
    "        print(f'No se pudo leer el dataset {ds}')\n",
    "        return None\n",
    "    \n",
    "    statsDir = crearDir(repDir, dsName)\n",
    "    excelFile = f'{statsDir}/Stats_{params[\"redTipo\"]}_{params[\"outputs\"]}_{datetime.today().strftime(\"%Y%m\")}.xlsx'\n",
    "    \n",
    "    if params['record'] and not os.path.exists(excelFile):\n",
    "        writer = pd.ExcelWriter(excelFile, engine = 'xlsxwriter')\n",
    "        keys_values = params.items()\n",
    "        strParams = {str(key): str(value) for key, value in keys_values}\n",
    "        strParams['dsName'] = dsName\n",
    "        pd.DataFrame(strParams,index=[0]).to_excel(writer, sheet_name = 'Informacion')          \n",
    "        writer.save()\n",
    "        \n",
    "        \n",
    "    return statsDir, excelFile, dataset            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03005a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCM(logs):\n",
    "    lKeys = list(logs.keys())\n",
    "    \n",
    "    try:\n",
    "        TN = int(logs[[x for x in lKeys if 'val_true_negatives' in x][0]])\n",
    "        TP = int(logs[[x for x in lKeys if 'val_true_positives' in x][0]])\n",
    "        FN = int(logs[[x for x in lKeys if 'val_false_negatives' in x][0]])\n",
    "        FP = int(logs[[x for x in lKeys if 'val_false_positives' in x][0]])\n",
    "    except:\n",
    "        print(f'\\nNo se pudo leer keys para la matriz de confucion en logs : {lKeys}')\n",
    "        print(f'Se intento leer: val_true_negatives,val_true_positives, val_false_negatives y val_false_positives')\n",
    "    \n",
    "       \n",
    "    y_true =  [0]*TN + [1]*TP + [1]*FN + [0]*FP\n",
    "    _y_pred = [0]*TN + [1]*TP + [0]*FN + [1]*FP\n",
    "    \n",
    "    return TN, FP, FN, TP, np.array(y_true), np.array(_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d2f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCB(Callback):\n",
    "    \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "    def __init__(self, val_ds=None, WANDB=True):     \n",
    "        self.val_ds = val_ds  \n",
    "        self.history = {}\n",
    "        self.wandb = WANDB\n",
    "   \n",
    "   \n",
    "    def on_epoch_end(self, epoch, logs={}):  \n",
    "        TN, FP, FN, TP, y_true, _y_pred = getCM(logs)\n",
    "        \n",
    "        self.history.setdefault('loss', []).append(logs['loss']) \n",
    "        self.history.setdefault('acc', []).append(logs['acc'])  \n",
    "        self.history.setdefault('val_loss', []).append(logs['val_loss']) \n",
    "        self.history.setdefault('val_acc', []).append(logs['val_acc']) \n",
    "        \n",
    "        \n",
    "        self.history.setdefault('val_TN', []).append(TN) \n",
    "        self.history.setdefault('val_FP', []).append(FP)\n",
    "        self.history.setdefault('val_FN', []).append(FN) \n",
    "        self.history.setdefault('val_TP', []).append(TP) \n",
    "        \n",
    "            \n",
    "        if self.wandb:\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                    preds=_y_pred, y_true=y_true,\n",
    "                                    class_names=[0,1]),                   \n",
    "                       'val_TN' :TN,'val_FN' :FN,'val_TP' :TP,'val_FP' :FP,\n",
    "                       'val_acc': logs['val_acc'],'loss' : logs['loss'],\n",
    "                       'val_loss': logs['val_loss'],'acc' : logs['acc']                  \n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1c5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearCallbacks(statsDir,dsName, params,run, metricas):\n",
    "    CB = metricas['callbacks']\n",
    "    \n",
    "    idModel = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = statsDir + '/Model_{epoch:02d}_' + f'{params[\"redTipo\"]}_{params[\"outputs\"]}_{idModel}.hdf5' \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                     \n",
    "                                                     verbose=1)\n",
    "    \n",
    "    # Iniciamos WANDB\n",
    "    if params['record']:        \n",
    "        CB.append(cp_callback)   \n",
    "     \n",
    "    if params['WANDB']:\n",
    "        config = dict(learning_rate=params['lr'], epochs = params['epocas'],\n",
    "             batch_size =params['batch'],architecture=\"CNN\", \n",
    "             num_classes = params['num_class'],)\n",
    "        wandb.init(project=f'{params[\"Proyect\"]}-({params[\"redTipo\"]}-{params[\"outputs\"]}-{len(params[\"inputs\"])})',            \n",
    "                   config=config,\n",
    "                   name= f'Ex_{dsName}_({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})_{idModel}')   \n",
    "                                               \n",
    "    return CB, idModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(params,path_imagenes, path_base ,products, times):   \n",
    "    idProject = datetime.today().strftime(\"%Y%m%d_%H\")\n",
    "    params[\"Proyect\"] = f'{params[\"Proyect\"]}-{idProject}'\n",
    "        \n",
    "    if '.csv' in params[\"dsDir\"]:\n",
    "        ds_files = [params[\"dsDir\"]]\n",
    "    else:\n",
    "        ds_files = [f'{params[\"dsDir\"]}{e}' for e in os.listdir(params[\"dsDir\"])]\n",
    "        ds_files = [e for e in ds_files if '.csv' in e]\n",
    "        \n",
    "    maxDS = len(ds_files) if params['maxDS']==-1 else params['maxDS']\n",
    "    print(f'Cantidad de datasets encontrados : {len(ds_files)}')    \n",
    "    print(f'Datasets a usar : {maxDS}')    \n",
    "        \n",
    "           \n",
    "    \n",
    "    all_result = []      \n",
    "    \n",
    "    repDir = crearDir(f'{path_base}/Archivos/Resultados', params[\"redTipo\"])\n",
    "    repDir = crearDir(repDir, params[\"Proyect\"])\n",
    "    \n",
    "    print(f'DIRECTORIO BASE : {repDir}')\n",
    "    \n",
    "    # Una iteracion por cada dataset existente\n",
    "    ds_i = 0\n",
    "    \n",
    "    \n",
    "    for ds in ds_files[ds_i:maxDS+1]:\n",
    "        ds_i += 1       \n",
    "        \n",
    "        # Crear carpeta para DS y archivo de stats y el ds\n",
    "        statsDir , statsFile , dataset = inicializarVariables(repDir, params,ds)         \n",
    "            \n",
    "        resultados = []    \n",
    "        for run in range(params['runs']): \n",
    "            print('__________________________________________________')\n",
    "            print(f'DATASET : {ds_i}/{maxDS}')            \n",
    "            print('__________________________________________________')\n",
    "            print(f'Inicio de la prueba N°: {(run+1)}/{params[\"runs\"]}') \n",
    "            print(f'Dataset: {ds.split(\".\")[0]}')\n",
    "            print(f'- Batch size:  {params[\"batch\"]}')\n",
    "            print('__________________________________________________')\n",
    "            \n",
    "           \n",
    "            # Modelo \n",
    "            model = crearModelo(params,run) \n",
    "            metricas = getMetrics(params['redTipo'], params['lr'], params['paciencia'])\n",
    "            model.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "            \n",
    "            # Dataset        \n",
    "            train_dataset, val_dataset = splitDataset(params,run, dataset, path_imagenes, products, times, params['val_split'])\n",
    "      \n",
    "            # Creamos los callbacks\n",
    "            CB, idModel = crearCallbacks(statsDir,ds.split('/')[-1][:-4], params,run, metricas)           \n",
    "\n",
    "            if params['redTipo'] == 'Clasificacion':\n",
    "                hist =  CustomCB(val_dataset, params['WANDB'])\n",
    "                #CB.append(hist)\n",
    "        \n",
    "            \n",
    "            #Entrenamos\n",
    "            history = model.fit(train_dataset,batch_size=params['batch'],                            \n",
    "                                epochs=params['epocas'],callbacks=CB,\n",
    "                                validation_data=val_dataset,\n",
    "                                validation_batch_size=params['batch'],\n",
    "                                verbose=1)\n",
    "            if params['redTipo'] != 'Clasificacion': \n",
    "                HistTemp = history\n",
    "            else:\n",
    "                HistTemp = history#hist\n",
    "                \n",
    "            resultados.append(HistTemp.history)\n",
    "            \n",
    "            # Guardamos las estadisticas\n",
    "            if params['record']:                \n",
    "                with pd.ExcelWriter(statsFile, mode=\"a\", engine=\"openpyxl\", if_sheet_exists='overlay') as writer:                    \n",
    "                    tempDF = pd.DataFrame(HistTemp.history)\n",
    "                    if params['redTipo'] == 'Clasificacion':\n",
    "                        tempDF.columns = ['loss', 'acc', 'TP', 'TN', 'FP','FN','val_loss','val_acc','val_TP','val_TN','val_FP','val_FN']\n",
    "                    tempDF.to_excel(writer,startrow=0,\n",
    "                                    sheet_name=f'{run}-({params[\"canales\"][run]}-{params[\"tiempos\"][run]}-{params[\"margen\"][run]})-{idModel}')\n",
    "            if params['WANDB']:\n",
    "                wandb.finish()\n",
    "        \n",
    "        all_result.append(resultados)\n",
    "        \n",
    "    return all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9ce9bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "174a6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Definimos las varibles para las iteraciones\n",
    "Los parametros que van a cambiar son:\n",
    "- Canales (products)\n",
    "- Tiempos (Minutos de las imagenes)\n",
    "- margen\n",
    "\"\"\"\n",
    "\n",
    "modelTipo = 'Clasificacion'\n",
    "\n",
    "p_train = {\n",
    "            # Reportes\n",
    "          'Proyect'  : f'{modelTipo}_DUD_MEJOR_UMB2', # TesisDiego\n",
    "          'record'   : True,  # Grabar los resultados en  excels    \n",
    "          'WANDB'    : False, # Grabar los resultados en WANDB\n",
    "    \n",
    "            # Datos del modelo\n",
    "          'redTipo'  : modelTipo, # Clasificacion / Regresion\n",
    "          'inputs'   : ['imagen','umb2'], #altura]\n",
    "          'outputs'  : 'clase',       # clase / dato\n",
    "          'num_class': 2,             # Solo se usa en el modelo de clasificacion\n",
    "          'meanMatrizImagen' : False, # True -> Usa una conv2d, caso contrario conv3d                     \n",
    "          \n",
    "        \n",
    "            # Variables del entrenamiento          \n",
    "          'lr'       : 0.001,\n",
    "          'batch'    : 32,        \n",
    "          'val_split': 1,     # ¡¡ Si dsVal existe, este valor se ignora !!\n",
    "          'epocas'   : 20,  \n",
    "          'paciencia': 15,        # 0 = No paciencia  \n",
    "    \n",
    "           # Dataset\n",
    "          'dsDir'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Entrenamiento/SplitConDA_20C_DM_V2/',  #SplitConDA_DM_DFAD_DUD\n",
    "          'dsVal'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Validacion/ClaseV2_DUD_ValidacionDS.csv',\n",
    "          'maxDS'    : -1,      #-1 = Todos\n",
    "          'dataset'  : 1,     # 1 = 100% del ds\n",
    "          'DA'       : True,  # Usaulmente para clasificacion\n",
    "                    \n",
    "           # Hiper parametros \n",
    "          'canales'  : [3],\n",
    "          'tiempos'  : [4],\n",
    "          'margen'   : [30],\n",
    "          'runs'     : 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56ff0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALDIACION : 3207\n",
      "--------------------------------------------------\n",
      "Cantidad de datasets : 2\n",
      "TRAIN: 6008\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>...</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>umb1</th>\n",
       "      <th>umb2</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "      <th>DA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37890</td>\n",
       "      <td>PICHARI</td>\n",
       "      <td>47E880E2</td>\n",
       "      <td>539</td>\n",
       "      <td>772</td>\n",
       "      <td>-73.83952</td>\n",
       "      <td>-12.52219</td>\n",
       "      <td>570.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2021-11-06-00</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>539--772--2021-11-06-00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37891</td>\n",
       "      <td>SAN PABLO</td>\n",
       "      <td>4729658E</td>\n",
       "      <td>607</td>\n",
       "      <td>799</td>\n",
       "      <td>-72.61992</td>\n",
       "      <td>-13.02506</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2021-12-14-04</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>607--799--2021-12-14-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0     nombre    codigo   XO   XA  longitud  \\\n",
       "0             0       37890    PICHARI  47E880E2  539  772 -73.83952   \n",
       "1             1       37891  SAN PABLO  4729658E  607  799 -72.61992   \n",
       "\n",
       "    latitud  altura  dato  ...    99%  75%  umb1  umb2          fecha  \\\n",
       "0 -12.52219   570.0   0.1  ...  6.319  0.0   4.2  24.0  2021-11-06-00   \n",
       "1 -13.02506  1237.0   0.2  ...  1.600  0.0   2.6   6.9  2021-12-14-04   \n",
       "\n",
       "       flag flagV2                   imagen clase  DA  \n",
       "0  M0000002    C01  539--772--2021-11-06-00     0   0  \n",
       "1  M0000002    C01  607--799--2021-12-14-04     0   0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forma del DS\n",
    "ds_files = [f'{p_train[\"dsDir\"]}{e}' for e in os.listdir(p_train[\"dsDir\"])]\n",
    "ds_files = [e for e in ds_files if '.csv' in e]\n",
    "if p_train['dsVal']:\n",
    "    dfVal = pd.read_csv(p_train['dsVal'])\n",
    "    print(f'VALDIACION : {len(dfVal)}')\n",
    "    print('--------------------------------------------------')\n",
    "print(f'Cantidad de datasets : {len(ds_files)}')  \n",
    "tempDF = pd.read_csv(ds_files[0])\n",
    "print(f'TRAIN: {len(tempDF)}')\n",
    "tempDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e75fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datasets encontrados : 2\n",
      "Datasets a usar : 2\n",
      "DIRECTORIO BASE : C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\n",
      "__________________________________________________\n",
      "DATASET : 1/2\n",
      "__________________________________________________\n",
      "Inicio de la prueba N°: 1/1\n",
      "Dataset: C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Dataset/Clasificacion/Entrenamiento/SplitConDA_20C_DM_V2/CLASE_TrainDS_0\n",
      "- Batch size:  32\n",
      "__________________________________________________\n",
      "Creando modelo 3D\n",
      "HP :(T:4 - M:30 - C:3) y  tipo (Clasificacion)\n",
      "Tamaño del dataset: Train 6008  - Val 3207\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6865 - acc: 0.5176 - true_positives_7: 1161.0000 - true_negatives_7: 1949.0000 - false_positives_7: 1055.0000 - false_negatives_7: 1843.0000\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_01_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 246s 1s/step - loss: 0.6865 - acc: 0.5176 - true_positives_7: 1161.0000 - true_negatives_7: 1949.0000 - false_positives_7: 1055.0000 - false_negatives_7: 1843.0000 - val_loss: 0.6486 - val_acc: 0.8394 - val_true_positives_7: 2673.0000 - val_true_negatives_7: 19.0000 - val_false_positives_7: 31.0000 - val_false_negatives_7: 484.0000\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6814 - acc: 0.5459 - true_positives_7: 1720.0000 - true_negatives_7: 1560.0000 - false_positives_7: 1444.0000 - false_negatives_7: 1284.0000\n",
      "Epoch 2: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_02_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 289s 2s/step - loss: 0.6814 - acc: 0.5459 - true_positives_7: 1720.0000 - true_negatives_7: 1560.0000 - false_positives_7: 1444.0000 - false_negatives_7: 1284.0000 - val_loss: 0.5446 - val_acc: 0.8834 - val_true_positives_7: 2821.0000 - val_true_negatives_7: 12.0000 - val_false_positives_7: 38.0000 - val_false_negatives_7: 336.0000\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6851 - acc: 0.5261 - true_positives_7: 1350.0000 - true_negatives_7: 1811.0000 - false_positives_7: 1193.0000 - false_negatives_7: 1654.0000\n",
      "Epoch 3: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_03_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 319s 2s/step - loss: 0.6851 - acc: 0.5261 - true_positives_7: 1350.0000 - true_negatives_7: 1811.0000 - false_positives_7: 1193.0000 - false_negatives_7: 1654.0000 - val_loss: 0.6881 - val_acc: 0.6829 - val_true_positives_7: 2164.0000 - val_true_negatives_7: 26.0000 - val_false_positives_7: 24.0000 - val_false_negatives_7: 993.0000\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6782 - acc: 0.5771 - true_positives_7: 1879.0000 - true_negatives_7: 1588.0000 - false_positives_7: 1416.0000 - false_negatives_7: 1125.0000\n",
      "Epoch 4: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_04_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 336s 2s/step - loss: 0.6782 - acc: 0.5771 - true_positives_7: 1879.0000 - true_negatives_7: 1588.0000 - false_positives_7: 1416.0000 - false_negatives_7: 1125.0000 - val_loss: 0.5981 - val_acc: 0.7805 - val_true_positives_7: 2478.0000 - val_true_negatives_7: 25.0000 - val_false_positives_7: 25.0000 - val_false_negatives_7: 679.0000\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6731 - acc: 0.5598 - true_positives_7: 1863.0000 - true_negatives_7: 1500.0000 - false_positives_7: 1504.0000 - false_negatives_7: 1141.0000\n",
      "Epoch 5: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_05_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 337s 2s/step - loss: 0.6731 - acc: 0.5598 - true_positives_7: 1863.0000 - true_negatives_7: 1500.0000 - false_positives_7: 1504.0000 - false_negatives_7: 1141.0000 - val_loss: 0.6049 - val_acc: 0.7265 - val_true_positives_7: 2304.0000 - val_true_negatives_7: 26.0000 - val_false_positives_7: 24.0000 - val_false_negatives_7: 853.0000\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6683 - acc: 0.5576 - true_positives_7: 1814.0000 - true_negatives_7: 1536.0000 - false_positives_7: 1468.0000 - false_negatives_7: 1190.0000\n",
      "Epoch 6: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_06_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 341s 2s/step - loss: 0.6683 - acc: 0.5576 - true_positives_7: 1814.0000 - true_negatives_7: 1536.0000 - false_positives_7: 1468.0000 - false_negatives_7: 1190.0000 - val_loss: 0.5997 - val_acc: 0.7025 - val_true_positives_7: 2227.0000 - val_true_negatives_7: 26.0000 - val_false_positives_7: 24.0000 - val_false_negatives_7: 930.0000\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6641 - acc: 0.5832 - true_positives_7: 1757.0000 - true_negatives_7: 1747.0000 - false_positives_7: 1257.0000 - false_negatives_7: 1247.0000\n",
      "Epoch 7: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_07_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 347s 2s/step - loss: 0.6641 - acc: 0.5832 - true_positives_7: 1757.0000 - true_negatives_7: 1747.0000 - false_positives_7: 1257.0000 - false_negatives_7: 1247.0000 - val_loss: 0.5963 - val_acc: 0.6470 - val_true_positives_7: 2049.0000 - val_true_negatives_7: 26.0000 - val_false_positives_7: 24.0000 - val_false_negatives_7: 1108.0000\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6605 - acc: 0.6120 - true_positives_7: 1722.0000 - true_negatives_7: 1955.0000 - false_positives_7: 1049.0000 - false_negatives_7: 1282.0000\n",
      "Epoch 8: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_08_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 370s 2s/step - loss: 0.6605 - acc: 0.6120 - true_positives_7: 1722.0000 - true_negatives_7: 1955.0000 - false_positives_7: 1049.0000 - false_negatives_7: 1282.0000 - val_loss: 0.6022 - val_acc: 0.6152 - val_true_positives_7: 1947.0000 - val_true_negatives_7: 26.0000 - val_false_positives_7: 24.0000 - val_false_negatives_7: 1210.0000\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6565 - acc: 0.6170 - true_positives_7: 1670.0000 - true_negatives_7: 2037.0000 - false_positives_7: 967.0000 - false_negatives_7: 1334.0000\n",
      "Epoch 9: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_09_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 322s 2s/step - loss: 0.6565 - acc: 0.6170 - true_positives_7: 1670.0000 - true_negatives_7: 2037.0000 - false_positives_7: 967.0000 - false_negatives_7: 1334.0000 - val_loss: 0.6004 - val_acc: 0.6080 - val_true_positives_7: 1924.0000 - val_true_negatives_7: 26.0000 - val_false_positives_7: 24.0000 - val_false_negatives_7: 1233.0000\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6538 - acc: 0.6362 - true_positives_7: 1652.0000 - true_negatives_7: 2170.0000 - false_positives_7: 834.0000 - false_negatives_7: 1352.0000\n",
      "Epoch 10: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_10_Clasificacion_clase_20220601_202239.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 326s 2s/step - loss: 0.6538 - acc: 0.6362 - true_positives_7: 1652.0000 - true_negatives_7: 2170.0000 - false_positives_7: 834.0000 - false_negatives_7: 1352.0000 - val_loss: 0.5965 - val_acc: 0.6068 - val_true_positives_7: 1915.0000 - val_true_negatives_7: 31.0000 - val_false_positives_7: 19.0000 - val_false_negatives_7: 1242.0000\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6516 - acc: 0.6345 - true_positives_7: 1624.0000 - true_negatives_7: 2188.0000 - false_positives_7: 816.0000 - false_negatives_7: 1380.0000\n",
      "Epoch 11: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_11_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 381s 2s/step - loss: 0.6516 - acc: 0.6345 - true_positives_7: 1624.0000 - true_negatives_7: 2188.0000 - false_positives_7: 816.0000 - false_negatives_7: 1380.0000 - val_loss: 0.5932 - val_acc: 0.6068 - val_true_positives_7: 1915.0000 - val_true_negatives_7: 31.0000 - val_false_positives_7: 19.0000 - val_false_negatives_7: 1242.0000\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6486 - acc: 0.6358 - true_positives_7: 1613.0000 - true_negatives_7: 2207.0000 - false_positives_7: 797.0000 - false_negatives_7: 1391.0000\n",
      "Epoch 12: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_12_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 321s 2s/step - loss: 0.6486 - acc: 0.6358 - true_positives_7: 1613.0000 - true_negatives_7: 2207.0000 - false_positives_7: 797.0000 - false_negatives_7: 1391.0000 - val_loss: 0.5864 - val_acc: 0.6068 - val_true_positives_7: 1915.0000 - val_true_negatives_7: 31.0000 - val_false_positives_7: 19.0000 - val_false_negatives_7: 1242.0000\n",
      "Epoch 13/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6465 - acc: 0.6335 - true_positives_7: 1586.0000 - true_negatives_7: 2220.0000 - false_positives_7: 784.0000 - false_negatives_7: 1418.0000\n",
      "Epoch 13: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_13_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 285s 2s/step - loss: 0.6465 - acc: 0.6335 - true_positives_7: 1586.0000 - true_negatives_7: 2220.0000 - false_positives_7: 784.0000 - false_negatives_7: 1418.0000 - val_loss: 0.5844 - val_acc: 0.5828 - val_true_positives_7: 1833.0000 - val_true_negatives_7: 36.0000 - val_false_positives_7: 14.0000 - val_false_negatives_7: 1324.0000\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6433 - acc: 0.6355 - true_positives_7: 1582.0000 - true_negatives_7: 2236.0000 - false_positives_7: 768.0000 - false_negatives_7: 1422.0000\n",
      "Epoch 14: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_14_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 378s 2s/step - loss: 0.6433 - acc: 0.6355 - true_positives_7: 1582.0000 - true_negatives_7: 2236.0000 - false_positives_7: 768.0000 - false_negatives_7: 1422.0000 - val_loss: 0.5737 - val_acc: 0.5828 - val_true_positives_7: 1833.0000 - val_true_negatives_7: 36.0000 - val_false_positives_7: 14.0000 - val_false_negatives_7: 1324.0000\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6399 - acc: 0.6342 - true_positives_7: 1572.0000 - true_negatives_7: 2238.0000 - false_positives_7: 766.0000 - false_negatives_7: 1432.0000\n",
      "Epoch 15: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_15_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 324s 2s/step - loss: 0.6399 - acc: 0.6342 - true_positives_7: 1572.0000 - true_negatives_7: 2238.0000 - false_positives_7: 766.0000 - false_negatives_7: 1432.0000 - val_loss: 0.5558 - val_acc: 0.6673 - val_true_positives_7: 2110.0000 - val_true_negatives_7: 30.0000 - val_false_positives_7: 20.0000 - val_false_negatives_7: 1047.0000\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6370 - acc: 0.6418 - true_positives_7: 1618.0000 - true_negatives_7: 2238.0000 - false_positives_7: 766.0000 - false_negatives_7: 1386.0000\n",
      "Epoch 16: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_16_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 311s 2s/step - loss: 0.6370 - acc: 0.6418 - true_positives_7: 1618.0000 - true_negatives_7: 2238.0000 - false_positives_7: 766.0000 - false_negatives_7: 1386.0000 - val_loss: 0.5463 - val_acc: 0.6975 - val_true_positives_7: 2206.0000 - val_true_negatives_7: 31.0000 - val_false_positives_7: 19.0000 - val_false_negatives_7: 951.0000\n",
      "Epoch 17/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6340 - acc: 0.6460 - true_positives_7: 1700.0000 - true_negatives_7: 2181.0000 - false_positives_7: 823.0000 - false_negatives_7: 1304.0000\n",
      "Epoch 17: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_0\\Model_17_Clasificacion_clase_20220601_202239.hdf5\n",
      "188/188 [==============================] - 314s 2s/step - loss: 0.6340 - acc: 0.6460 - true_positives_7: 1700.0000 - true_negatives_7: 2181.0000 - false_positives_7: 823.0000 - false_negatives_7: 1304.0000 - val_loss: 0.5300 - val_acc: 0.7611 - val_true_positives_7: 2411.0000 - val_true_negatives_7: 30.0000 - val_false_positives_7: 20.0000 - val_false_negatives_7: 746.0000\n",
      "__________________________________________________\n",
      "DATASET : 2/2\n",
      "__________________________________________________\n",
      "Inicio de la prueba N°: 1/1\n",
      "Dataset: C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Dataset/Clasificacion/Entrenamiento/SplitConDA_20C_DM_V2/CLASE_TrainDS_1\n",
      "- Batch size:  32\n",
      "__________________________________________________\n",
      "Creando modelo 3D\n",
      "HP :(T:4 - M:30 - C:3) y  tipo (Clasificacion)\n",
      "Tamaño del dataset: Train 6008  - Val 3207\n",
      "Epoch 1/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6907 - acc: 0.5243 - true_positives_8: 1034.0000 - true_negatives_8: 2116.0000 - false_positives_8: 888.0000 - false_negatives_8: 1970.0000\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_01_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 312s 2s/step - loss: 0.6907 - acc: 0.5243 - true_positives_8: 1034.0000 - true_negatives_8: 2116.0000 - false_positives_8: 888.0000 - false_negatives_8: 1970.0000 - val_loss: 0.6636 - val_acc: 0.7128 - val_true_positives_8: 2260.0000 - val_true_negatives_8: 26.0000 - val_false_positives_8: 24.0000 - val_false_negatives_8: 897.0000\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6814 - acc: 0.5434 - true_positives_8: 1229.0000 - true_negatives_8: 2036.0000 - false_positives_8: 968.0000 - false_negatives_8: 1775.0000\n",
      "Epoch 2: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_02_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 311s 2s/step - loss: 0.6814 - acc: 0.5434 - true_positives_8: 1229.0000 - true_negatives_8: 2036.0000 - false_positives_8: 968.0000 - false_negatives_8: 1775.0000 - val_loss: 0.6693 - val_acc: 0.6289 - val_true_positives_8: 1991.0000 - val_true_negatives_8: 26.0000 - val_false_positives_8: 24.0000 - val_false_negatives_8: 1166.0000\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - ETA: 0s - loss: 0.6773 - acc: 0.5744 - true_positives_8: 1549.0000 - true_negatives_8: 1902.0000 - false_positives_8: 1102.0000 - false_negatives_8: 1455.0000\n",
      "Epoch 3: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_03_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 390s 2s/step - loss: 0.6773 - acc: 0.5744 - true_positives_8: 1549.0000 - true_negatives_8: 1902.0000 - false_positives_8: 1102.0000 - false_negatives_8: 1455.0000 - val_loss: 0.7433 - val_acc: 0.4415 - val_true_positives_8: 1377.0000 - val_true_negatives_8: 39.0000 - val_false_positives_8: 11.0000 - val_false_negatives_8: 1780.0000\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6743 - acc: 0.5722 - true_positives_8: 1658.0000 - true_negatives_8: 1780.0000 - false_positives_8: 1224.0000 - false_negatives_8: 1346.0000\n",
      "Epoch 4: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_04_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 304s 2s/step - loss: 0.6743 - acc: 0.5722 - true_positives_8: 1658.0000 - true_negatives_8: 1780.0000 - false_positives_8: 1224.0000 - false_negatives_8: 1346.0000 - val_loss: 0.7296 - val_acc: 0.4855 - val_true_positives_8: 1519.0000 - val_true_negatives_8: 38.0000 - val_false_positives_8: 12.0000 - val_false_negatives_8: 1638.0000\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6735 - acc: 0.5636 - true_positives_8: 1575.0000 - true_negatives_8: 1811.0000 - false_positives_8: 1193.0000 - false_negatives_8: 1429.0000\n",
      "Epoch 5: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_05_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 269s 1s/step - loss: 0.6735 - acc: 0.5636 - true_positives_8: 1575.0000 - true_negatives_8: 1811.0000 - false_positives_8: 1193.0000 - false_negatives_8: 1429.0000 - val_loss: 0.6543 - val_acc: 0.5875 - val_true_positives_8: 1853.0000 - val_true_negatives_8: 31.0000 - val_false_positives_8: 19.0000 - val_false_negatives_8: 1304.0000\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6694 - acc: 0.5897 - true_positives_8: 1528.0000 - true_negatives_8: 2015.0000 - false_positives_8: 989.0000 - false_negatives_8: 1476.0000\n",
      "Epoch 6: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_06_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 337s 2s/step - loss: 0.6694 - acc: 0.5897 - true_positives_8: 1528.0000 - true_negatives_8: 2015.0000 - false_positives_8: 989.0000 - false_negatives_8: 1476.0000 - val_loss: 0.6544 - val_acc: 0.5566 - val_true_positives_8: 1749.0000 - val_true_negatives_8: 36.0000 - val_false_positives_8: 14.0000 - val_false_negatives_8: 1408.0000\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6633 - acc: 0.6122 - true_positives_8: 1552.0000 - true_negatives_8: 2126.0000 - false_positives_8: 878.0000 - false_negatives_8: 1452.0000\n",
      "Epoch 7: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_07_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 375s 2s/step - loss: 0.6633 - acc: 0.6122 - true_positives_8: 1552.0000 - true_negatives_8: 2126.0000 - false_positives_8: 878.0000 - false_negatives_8: 1452.0000 - val_loss: 0.7977 - val_acc: 0.3315 - val_true_positives_8: 1021.0000 - val_true_negatives_8: 42.0000 - val_false_positives_8: 8.0000 - val_false_negatives_8: 2136.0000\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6596 - acc: 0.6040 - true_positives_8: 1562.0000 - true_negatives_8: 2067.0000 - false_positives_8: 937.0000 - false_negatives_8: 1442.0000\n",
      "Epoch 8: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_08_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 448s 2s/step - loss: 0.6596 - acc: 0.6040 - true_positives_8: 1562.0000 - true_negatives_8: 2067.0000 - false_positives_8: 937.0000 - false_negatives_8: 1442.0000 - val_loss: 0.8049 - val_acc: 0.3312 - val_true_positives_8: 1021.0000 - val_true_negatives_8: 41.0000 - val_false_positives_8: 9.0000 - val_false_negatives_8: 2136.0000\n",
      "Epoch 9/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6544 - acc: 0.6029 - true_positives_8: 1546.0000 - true_negatives_8: 2076.0000 - false_positives_8: 928.0000 - false_negatives_8: 1458.0000\n",
      "Epoch 9: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_09_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 313s 2s/step - loss: 0.6544 - acc: 0.6029 - true_positives_8: 1546.0000 - true_negatives_8: 2076.0000 - false_positives_8: 928.0000 - false_negatives_8: 1458.0000 - val_loss: 0.8140 - val_acc: 0.3536 - val_true_positives_8: 1093.0000 - val_true_negatives_8: 41.0000 - val_false_positives_8: 9.0000 - val_false_negatives_8: 2064.0000\n",
      "Epoch 10/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6514 - acc: 0.5985 - true_positives_8: 1583.0000 - true_negatives_8: 2013.0000 - false_positives_8: 991.0000 - false_negatives_8: 1421.0000\n",
      "Epoch 10: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_10_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 312s 2s/step - loss: 0.6514 - acc: 0.5985 - true_positives_8: 1583.0000 - true_negatives_8: 2013.0000 - false_positives_8: 991.0000 - false_negatives_8: 1421.0000 - val_loss: 0.7808 - val_acc: 0.3542 - val_true_positives_8: 1097.0000 - val_true_negatives_8: 39.0000 - val_false_positives_8: 11.0000 - val_false_negatives_8: 2060.0000\n",
      "Epoch 11/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6491 - acc: 0.6067 - true_positives_8: 1615.0000 - true_negatives_8: 2030.0000 - false_positives_8: 974.0000 - false_negatives_8: 1389.0000\n",
      "Epoch 11: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_11_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 310s 2s/step - loss: 0.6491 - acc: 0.6067 - true_positives_8: 1615.0000 - true_negatives_8: 2030.0000 - false_positives_8: 974.0000 - false_negatives_8: 1389.0000 - val_loss: 0.8941 - val_acc: 0.2822 - val_true_positives_8: 864.0000 - val_true_negatives_8: 41.0000 - val_false_positives_8: 9.0000 - val_false_negatives_8: 2293.0000\n",
      "Epoch 12/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6375 - acc: 0.6233 - true_positives_8: 1717.0000 - true_negatives_8: 2028.0000 - false_positives_8: 976.0000 - false_negatives_8: 1287.0000\n",
      "Epoch 12: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_12_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 311s 2s/step - loss: 0.6375 - acc: 0.6233 - true_positives_8: 1717.0000 - true_negatives_8: 2028.0000 - false_positives_8: 976.0000 - false_negatives_8: 1287.0000 - val_loss: 0.8383 - val_acc: 0.3377 - val_true_positives_8: 1043.0000 - val_true_negatives_8: 40.0000 - val_false_positives_8: 10.0000 - val_false_negatives_8: 2114.0000\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - ETA: 0s - loss: 0.6325 - acc: 0.6273 - true_positives_8: 1730.0000 - true_negatives_8: 2039.0000 - false_positives_8: 965.0000 - false_negatives_8: 1274.0000\n",
      "Epoch 13: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_13_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 309s 2s/step - loss: 0.6325 - acc: 0.6273 - true_positives_8: 1730.0000 - true_negatives_8: 2039.0000 - false_positives_8: 965.0000 - false_negatives_8: 1274.0000 - val_loss: 0.7803 - val_acc: 0.4144 - val_true_positives_8: 1294.0000 - val_true_negatives_8: 35.0000 - val_false_positives_8: 15.0000 - val_false_negatives_8: 1863.0000\n",
      "Epoch 14/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6262 - acc: 0.6305 - true_positives_8: 1794.0000 - true_negatives_8: 1994.0000 - false_positives_8: 1010.0000 - false_negatives_8: 1210.0000\n",
      "Epoch 14: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_14_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 307s 2s/step - loss: 0.6262 - acc: 0.6305 - true_positives_8: 1794.0000 - true_negatives_8: 1994.0000 - false_positives_8: 1010.0000 - false_negatives_8: 1210.0000 - val_loss: 0.7817 - val_acc: 0.4344 - val_true_positives_8: 1359.0000 - val_true_negatives_8: 34.0000 - val_false_positives_8: 16.0000 - val_false_negatives_8: 1798.0000\n",
      "Epoch 15/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6234 - acc: 0.6313 - true_positives_8: 1813.0000 - true_negatives_8: 1980.0000 - false_positives_8: 1024.0000 - false_negatives_8: 1191.0000\n",
      "Epoch 15: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_15_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 269s 1s/step - loss: 0.6234 - acc: 0.6313 - true_positives_8: 1813.0000 - true_negatives_8: 1980.0000 - false_positives_8: 1024.0000 - false_negatives_8: 1191.0000 - val_loss: 0.6893 - val_acc: 0.5320 - val_true_positives_8: 1676.0000 - val_true_negatives_8: 30.0000 - val_false_positives_8: 20.0000 - val_false_negatives_8: 1481.0000\n",
      "Epoch 16/20\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6152 - acc: 0.6418 - true_positives_8: 1876.0000 - true_negatives_8: 1980.0000 - false_positives_8: 1024.0000 - false_negatives_8: 1128.0000\n",
      "Epoch 16: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/NewTesis/Archivos/Resultados\\Clasificacion\\Clasificacion_DUD_MEJOR_UMB2-20220601_20\\CLASE_TrainDS_1\\Model_16_Clasificacion_clase_20220601_215513.hdf5\n",
      "188/188 [==============================] - 216s 1s/step - loss: 0.6152 - acc: 0.6418 - true_positives_8: 1876.0000 - true_negatives_8: 1980.0000 - false_positives_8: 1024.0000 - false_negatives_8: 1128.0000 - val_loss: 0.7010 - val_acc: 0.5382 - val_true_positives_8: 1696.0000 - val_true_negatives_8: 30.0000 - val_false_positives_8: 20.0000 - val_false_negatives_8: 1461.0000\n",
      "CPU times: total: 21h 54min 29s\n",
      "Wall time: 2h 57min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultados = trainModel(p_train,path_imagenes,path_base,products,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328e353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
