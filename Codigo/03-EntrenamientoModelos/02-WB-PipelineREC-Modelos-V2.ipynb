{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b716303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OBJETIVO : Definir y entrenar lso modelos. Ademas, recolectar los resultados para analizarlos mas adelante  \n",
    "V2 : Se agrega redes pre entrenadas\n",
    "\"\"\"\n",
    "Autor='Diego Paredes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443f8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shounen\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28142b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53969cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/FinalTesis/Tesis2-DiegoParedes'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH DEL PROYECTO \n",
    "\"\"\"\n",
    "with open('../../path_base.txt') as f:\n",
    "    path_base = f.read()\n",
    "path_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3336ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables generales\n",
    "\"\"\"\n",
    "path_imagenes = 'F:/GOES/'     \n",
    "\n",
    "products = ['C13','C07','C08']\n",
    "times   = ['10','20','30','40','50','00']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "#Limitamos el GPU, en caso se necesite\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "                                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c44d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConv3D_func(p,run,input_1, shape=()):   \n",
    "    \n",
    "    x = input_1\n",
    "    if p['normLayer'][run]:\n",
    "        x = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "        \n",
    "    x = tf.keras.layers.Conv3D(32, (3,3,3), input_shape=(shape),padding='same', activation='relu')(x)    \n",
    "    \n",
    "    \n",
    "    for iConv in range(p['cnn_cant'][run]):\n",
    "        units = p['cnn_units'][run][iConv]\n",
    "        maxPool = p['maxPool'][run][iConv]\n",
    "        droupout = p['droupout'][run][iConv]        \n",
    "        \n",
    "        x = tf.keras.layers.Conv3D(units, (3,3,3), padding='same', activation='relu')(x)\n",
    "        if maxPool:\n",
    "            x = tf.keras.layers.MaxPooling3D()(x)\n",
    "        if droupout:\n",
    "            x = tf.keras.layers.Dropout(droupout)(x)\n",
    "   \n",
    "    \n",
    "    output = tf.keras.layers.GlobalMaxPool3D()(x)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConv2D_func(p,run,input_1, shape=()):   \n",
    "    \n",
    "    x = input_1\n",
    "    if p['normLayer'][run]:\n",
    "        x = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(32, 3, input_shape=(shape),padding='same', activation='relu')(x)    \n",
    "    \n",
    "    \n",
    "    for iConv in range(p['cnn_cant'][run]):\n",
    "        units = p['cnn_units'][run][iConv]\n",
    "        maxPool = p['maxPool'][run][iConv]\n",
    "        droupout = p['droupout'][run][iConv]        \n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(units, 3, padding='same', activation='relu')(x)\n",
    "        if maxPool:\n",
    "            x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        if droupout:\n",
    "            x = tf.keras.layers.Dropout(droupout)(x)\n",
    "   \n",
    "    \n",
    "    output = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "    \n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f5f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConv2D(p,run):    \n",
    "    model = keras.Sequential()\n",
    "    shape = (p['margen'][run],p['margen'][run],p['canales'][run])\n",
    "        \n",
    "    if p['normLayer'][run]:\n",
    "        model.add(tf.keras.layers.Rescaling(1./65536))    \n",
    "        \n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3), input_shape=(p['margen'][run],p['margen'][run],p['canales'][run]),padding='same', activation='relu'))    \n",
    "    \n",
    "    for iConv in range(p['cnn_cant'][run]):\n",
    "        units = p['cnn_units'][run][iConv]\n",
    "        maxPool = p['maxPool'][run][iConv]\n",
    "        droupout = p['droupout'][run][iConv]        \n",
    "        \n",
    "        model.add(tf.keras.layers.Conv2D(units, (3,3), padding='same', activation='relu'))\n",
    "        if maxPool:\n",
    "            model.add(tf.keras.layers.MaxPooling2D())\n",
    "        if droupout:\n",
    "            model.add(tf.keras.layers.Dropout(droupout))       \n",
    "   \n",
    "    \n",
    "    model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3c75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPreTrainedModel(p, run, shape=()):\n",
    "    print('Se utilizara red pre entreanda : ',p['pre_trained'][run])\n",
    "    convnet = tf.keras.applications.ResNet50(weights='imagenet', include_top=False , input_shape=shape)\n",
    "    for capa in convnet.layers:\n",
    "        capa.trainable = False\n",
    "        \n",
    "    \n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f540a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(p,run, redTipo):        \n",
    "\n",
    "    shape = (p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run])    \n",
    "    print(shape)\n",
    "    if p['rnn_tipo'][run] == 'LSTM':\n",
    "        input_1 = tf.keras.layers.Input(shape=shape)\n",
    "        if p['pre_trained'][run]:\n",
    "            convnet= getPreTrainedModel(p, run)\n",
    "        else:\n",
    "            convnet = getConv2D(p,run)\n",
    "        timeD = tf.keras.layers.TimeDistributed(convnet)(input_1)\n",
    "        timeD = layers.TimeDistributed(tf.keras.layers.Flatten())(timeD)\n",
    "        _rnn =  tf.keras.layers.LSTM(p['rnn_units'][run])(timeD)\n",
    "        listConcat = [_rnn]\n",
    "    elif p['rnn_tipo'][run] == 'CONV3D':\n",
    "        input_1 = tf.keras.layers.Input(shape=shape)\n",
    "        convnet = getConv3D_func(p,run,input_1)    \n",
    "        listConcat = [convnet]        \n",
    "    elif p['rnn_tipo'][run] == 'CONV2D':\n",
    "        input_1 = tf.keras.layers.Input(shape=shape[1:])\n",
    "        if p['pre_trained'][run]:\n",
    "            convnet= getPreTrainedModel(p, run, shape=shape[1:])\n",
    "            convnet = tf.keras.layers.Flatten()(convnet.output)\n",
    "        else:\n",
    "            #convnet = getConv2D(p,run)\n",
    "            convnet = getConv2D_func(p,run,input_1,shape=shape[1:])   \n",
    "        listConcat = [convnet]   \n",
    "    else:\n",
    "        print(f\"ERROR: No se especifico un tipo de red correcto... {p['rnn_tipo'] }\")\n",
    "        \n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'][run])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][run][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat) \n",
    "    \n",
    "    \n",
    "    \n",
    "    dense_capas = [final]\n",
    "    for iDense in range(p['dense_cant'][run]):\n",
    "        units = p['dense_units'][run][iDense]\n",
    "        if p['dense_tipo'][run] == 'RELU':\n",
    "            dense_capas.append(tf.keras.layers.Dense(units=units, activation=tf.keras.activations.relu)(dense_capas[iDense]))\n",
    "        if p['dense_tipo'][run] == 'SELU':\n",
    "            dense_capas.append(tf.keras.layers.Dense(units=units, activation=tf.keras.activations.selu)(dense_capas[iDense]))\n",
    "    \n",
    "    # output\n",
    "    if redTipo == 'Regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_capas[-1])      \n",
    "    elif redTipo == 'Clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_capas[-1])\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(params, HP, run):\n",
    "    redTipo = params['redTipo']\n",
    "    paciencia = params['paciencia']\n",
    "    \n",
    "    lr = HP['lr'][run]    \n",
    "    \n",
    "    if redTipo == 'Clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)   \n",
    "        if HP['loss'][run] == 'binary_crossentropy':\n",
    "            loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        \n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif redTipo == 'Regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(img, DA):\n",
    "    # DA = 0 , imagen original\n",
    "    \n",
    "    # DA = 1 , flip horizontal (FH)\n",
    "    # DA = 2 , flip vertical (FV)\n",
    "    # DA = 3 , Flip diagonal (FV + FH)\n",
    "    # DA = 4 , Rotacion 90° (R90)\n",
    "    # DA = 5 , Rotacion 270° (R270)\n",
    "    # DA = 6 , FH + Rotacion 90° (FH + R90)\n",
    "    # DA = 7 , FH + Rotacion 270° (FH + R270)    \n",
    "    \n",
    "    \n",
    "    if DA == 1:\n",
    "        img= tf.image.flip_left_right(img)  # FH\n",
    "        return img\n",
    "    elif DA == 2:\n",
    "        return tf.image.flip_up_down(img)     # FV\n",
    "        return img\n",
    "    elif DA == 3:\n",
    "        img = tf.image.flip_left_right(img)   # FH\n",
    "        img= tf.image.flip_up_down(img)     # FV\n",
    "        return img\n",
    "    elif DA == 4:\n",
    "        img = tf.image.rot90(img, k=1)        # R90\n",
    "        #img = tfa.image.rotate(img, tf.constant(np.pi/2))        \n",
    "        return img\n",
    "    elif DA == 5:\n",
    "        img = tf.image.rot90(img, k=3)        # R270\n",
    "        #img = tfa.image.rotate(img, tf.constant(np.pi/2*3))\n",
    "        return img\n",
    "    elif DA == 6:\n",
    "        img = tf.image.flip_left_right(img)   # FH\n",
    "        img = tf.image.rot90(img, k=1)        # R90\n",
    "        #img = tfa.image.rotate(img, tf.constant(np.pi/2))\n",
    "        return img\n",
    "    elif DA == 7:\n",
    "        img = tf.image.flip_left_right(img)   # FH\n",
    "        img = tf.image.rot90(img, k=3)        # R270\n",
    "        #img = tfa.image.rotate(img, tf.constant(np.pi/2*3))  \n",
    "        return img\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d267ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sat_time_steps(n):    \n",
    "    if n==2:\n",
    "        return [5,0]\n",
    "    if n==3:\n",
    "        return [5,3,0]\n",
    "    if n==4:\n",
    "        return [5,3,2,0]\n",
    "    if n==5:\n",
    "        return [5,3,2,1,0]\n",
    "    if n==6:\n",
    "        return [5,4,3,2,1,0]\n",
    "    \n",
    "    return [5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value,p, run, path_base, DA=0, _3D=False, redTipo='Clasificacion'):\n",
    "    # imagenData[0] = XO\n",
    "    # imagenData[1] = XA\n",
    "    # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    \n",
    "    \n",
    "    size = int(p['margen'][run] / 2)\n",
    "    timeJoin = []\n",
    "    for j in get_sat_time_steps(p['tiempos'][run]):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "        \n",
    "        \n",
    "        #if p['normLayer'][run]:            \n",
    "        #    img_decoded = tf.cast(img_decoded, dtype=tf.float32) / tf.constant(65535, dtype=tf.float32)\n",
    "        \n",
    "        if DA:\n",
    "            img_decoded = applyDA(img_decoded, item['DA'])\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if _3D:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs'][run]) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs'][run]:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    if redTipo=='Regresion':\n",
    "        return tuple(itemL), float(value)\n",
    "    else:     \n",
    "        return tuple(itemL), int(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p,HP,run, path_imagenes):\n",
    "    \n",
    "    test = pd.read_csv(p['dsVal'])\n",
    "    train = pd.read_csv(p['dsTrain'])\n",
    "    \n",
    "    if p['dataset']:\n",
    "        print('Se escojera una parte del Dataset')\n",
    "        train =  train.sample(frac=p['dataset'])\n",
    "        test = test.sample(frac=p['dataset'])\n",
    "    \n",
    "               \n",
    "    inputsList = {}\n",
    "    inputsListTest = {}\n",
    "    \n",
    "    # Agregamos un atributo para indicar que el dato va realizar DA\n",
    "    if p['DA']:        \n",
    "        inputsList['DA'] = train['DA'].tolist() \n",
    "        \n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}') \n",
    "    \n",
    "    for inp in HP['inputs'][run]:\n",
    "        inputsList[inp] = train[inp].tolist()  \n",
    "        inputsListTest[inp] = test[inp].tolist()  \n",
    "    \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[HP['outputs'][run]].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsListTest),test[HP['outputs'][run]].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,HP,run,path_imagenes,p['DA'],p['meanMatrizImagen'],p['redTipo']))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,HP,run,path_imagenes,False,p['meanMatrizImagen'],p['redTipo']))\n",
    "       \n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])\n",
    "    val_dataset = val_dataset.batch(p['batch'])\n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5adbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearDir(path, newDir):\n",
    "    try:\n",
    "        pathT = os.path.join(path, newDir)\n",
    "        os.mkdir(pathT)\n",
    "        return pathT\n",
    "    except FileExistsError:\n",
    "        return pathT\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear el directorio: {newDir}\")\n",
    "        pritn(f'Path base: {path}')\n",
    "        pritn(f'Nuevo    : {newDir}')        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67dc36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iniciarProyect(path_base, params, HP):\n",
    "    repDir = crearDir(f'{path_base}/Archivos/Resultados', params[\"redTipo\"])   \n",
    "    repDir = crearDir(repDir, params[\"directory\"])\n",
    "    repDir = crearDir(repDir, params[\"Proyect\"])\n",
    "    \n",
    "    excelFile = f'{repDir}/Stats-{params[\"Proyect\"]}.xlsx'\n",
    "    \n",
    "    if params['record'] and not os.path.exists(excelFile):\n",
    "        writer = pd.ExcelWriter(excelFile, engine = 'xlsxwriter')\n",
    "        #joinParams = params | HP\n",
    "        keys_values = params.items()\n",
    "        strParams = {str(key): str(value) for key, value in keys_values}        \n",
    "        \n",
    "        pd.DataFrame(strParams,index=[0]).to_excel(writer, sheet_name = 'Informacion')\n",
    "        \n",
    "        pd.DataFrame(HP).to_excel(writer, sheet_name = 'Informacion',startrow=3)\n",
    "        \n",
    "        writer.save()  \n",
    "        \n",
    "        \n",
    "    return repDir , excelFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03005a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCM(logs):\n",
    "    lKeys = list(logs.keys())\n",
    "    \n",
    "    try:\n",
    "        TN = int(logs[[x for x in lKeys if 'val_true_negatives' in x][0]])\n",
    "        TP = int(logs[[x for x in lKeys if 'val_true_positives' in x][0]])\n",
    "        FN = int(logs[[x for x in lKeys if 'val_false_negatives' in x][0]])\n",
    "        FP = int(logs[[x for x in lKeys if 'val_false_positives' in x][0]])\n",
    "    except:\n",
    "        print(f'\\nNo se pudo leer keys para la matriz de confucion en logs : {lKeys}')\n",
    "        print(f'Se intento leer: val_true_negatives,val_true_positives, val_false_negatives y val_false_positives')\n",
    "    \n",
    "       \n",
    "    y_true =  [0]*TN + [1]*TP + [1]*FN + [0]*FP\n",
    "    _y_pred = [0]*TN + [1]*TP + [0]*FN + [1]*FP\n",
    "    \n",
    "    return TN, FP, FN, TP, np.array(y_true), np.array(_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d2f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCB(Callback):\n",
    "    \"\"\" Custom callback to compute metrics at the end of each training epoch\"\"\"\n",
    "    def __init__(self, val_ds=None, WANDB=True):     \n",
    "        self.val_ds = val_ds  \n",
    "        self.history = {}\n",
    "        self.wandb = WANDB\n",
    "   \n",
    "   \n",
    "    def on_epoch_end(self, epoch, logs={}):  \n",
    "        TN, FP, FN, TP, y_true, _y_pred = getCM(logs)\n",
    "        \n",
    "        self.history.setdefault('epoch', []).append(epoch)\n",
    "        \n",
    "        self.history.setdefault('loss', []).append(logs['loss']) \n",
    "        self.history.setdefault('acc', []).append(logs['acc'])  \n",
    "        self.history.setdefault('val_loss', []).append(logs['val_loss']) \n",
    "        self.history.setdefault('val_acc', []).append(logs['val_acc']) \n",
    "        \n",
    "        \n",
    "        self.history.setdefault('val_TN', []).append(TN) \n",
    "        self.history.setdefault('val_FP', []).append(FP)\n",
    "        self.history.setdefault('val_FN', []).append(FN) \n",
    "        self.history.setdefault('val_TP', []).append(TP) \n",
    "        \n",
    "            \n",
    "        if self.wandb:\n",
    "            wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                    preds=_y_pred, y_true=y_true,\n",
    "                                    class_names=[0,1]),                   \n",
    "                       'val_TN' :TN,'val_FN' :FN,'val_TP' :TP,'val_FP' :FP,\n",
    "                       'val_acc': logs['val_acc'],'loss' : logs['loss'],\n",
    "                       'val_loss': logs['val_loss'],'acc' : logs['acc']                  \n",
    "                      })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c5b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearCallbacks(statsDir, HP,run, metricas, p):\n",
    "    CB = metricas['callbacks']\n",
    "    \n",
    "    idModel = datetime.today().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_path = statsDir + '/Model_{epoch:02d}_' + f'{HP[\"rnn_tipo\"][run]}_{HP[\"outputs\"][run]}_{idModel}.hdf5' \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,verbose=1)\n",
    "    \n",
    "    # Iniciamos WANDB\n",
    "    if p['record']:        \n",
    "        CB.append(cp_callback)   \n",
    "     \n",
    "    if p['WANDB']:\n",
    "        config = dict(learning_rate=HP['lr'][run], epochs = p['epocas'],\n",
    "             batch_size = p['batch'],architecture=\"CNN\", \n",
    "             num_classes = 2,)\n",
    "        wandb.init(project=f'{p[\"Proyect\"]}',            \n",
    "                   config=config,\n",
    "                   name= f'Ex_({HP[\"canales\"][run]}-{HP[\"tiempos\"][run]}-{HP[\"margen\"][run]})_{idModel}')   \n",
    "                                               \n",
    "    return CB, idModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(path_base,path_imagenes,params,HP, criterio):        \n",
    "    \"\"\" Creamos los directorios para los reportes  \"\"\"             \n",
    "    repDir, statsFile  = iniciarProyect(path_base, params, HP)\n",
    "    print(f'DIRECTORIO BASE : {repDir}')\n",
    "    \n",
    "    \n",
    "    \"\"\" Comenzamos el entrenamiento \"\"\" \n",
    "    # Una iteracion por cada Hiperparametro (HP) que existe\n",
    "    ds_i = 0  \n",
    "    resultados = [] \n",
    "    for run in range(HP['runs']):\n",
    "        ds_i += 1                 \n",
    "        print(f'Inicio de la prueba N°: {(run+1)}/{HP[\"runs\"]}')         \n",
    "        print(f'- Nombre del Proyecto : {params[\"Proyect\"]}')\n",
    "        print(f'- Batch size          : {params[\"batch\"]}')\n",
    "        print(f'- Criterio {criterio} : {HP[criterio][run]}')\n",
    "        print('__________________________________________________')        \n",
    "        \n",
    "        \"\"\" LEEMOS EL DATASET A USAR  \"\"\"     \n",
    "        train_dataset, val_dataset = splitDataset(params,HP,run, path_imagenes)\n",
    "        \n",
    "\n",
    "        \"\"\" DEFINIMOS Y INICAMOS EL MODELO \"\"\"\n",
    "        model = crearModelo(HP,run,params['redTipo']) \n",
    "        metricas = getMetrics(params, HP, run)\n",
    "        model.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "        \n",
    "\n",
    "        \"\"\" CALLBACKS \"\"\"\n",
    "        CB, idModel = crearCallbacks(repDir, HP,run, metricas, params)           \n",
    "        if params['redTipo'] == 'Clasificacion':\n",
    "            hist =  CustomCB(val_dataset, params['WANDB'])\n",
    "\n",
    "            \n",
    "        \"\"\" ENTRENAMIENTO \"\"\"\n",
    "        history = model.fit(train_dataset,batch_size=params['batch'],                            \n",
    "                            epochs=params['epocas'],callbacks=CB,\n",
    "                            validation_data=val_dataset,\n",
    "                            validation_batch_size=params['batch'],\n",
    "                            verbose=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\" GUARDAMOS REPORTES \"\"\"                \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            resultados.append(history.history)\n",
    "            # Guardamos las estadisticas\n",
    "            if params['record']:                \n",
    "                with pd.ExcelWriter(statsFile, mode=\"a\", engine=\"openpyxl\", if_sheet_exists='overlay') as writer:                    \n",
    "                    tempDF = pd.DataFrame(history.history)\n",
    "                    if params['redTipo'] == 'Clasificacion':\n",
    "                        tempDF.columns = ['loss', 'acc', 'TP', 'TN', 'FP','FN',\n",
    "                                          'val_loss','val_acc','val_TP','val_TN','val_FP','val_FN']\n",
    "                    tempDF.to_excel(writer,startrow=0,\n",
    "                                    sheet_name=f'{run}-{idModel}')\n",
    "            if params['WANDB']:\n",
    "                wandb.finish()            \n",
    "        except Exception as e:\n",
    "            print(f'ERROR: No se pudo guardar los resulatdos test: {run}', str(e))\n",
    "            try:\n",
    "                with open(f'{repDir}/Test-{run}-{idModel}.csv', 'wb') as file_pi:\n",
    "                    pickle.dump(history.history, file_pi)\n",
    "            except Exception:\n",
    "                print(traceback.format_exc())\n",
    "                print(f'ERROR: No se pudo guardar el hist temporal')\n",
    "                return history\n",
    "            \n",
    "        \n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83a42ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregar datos altos a vajos en malos C02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26cf2f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALDIACION : 3101\n",
      "--------------------------------------------------\n",
      "TRAIN: 9824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>...</th>\n",
       "      <th>umb2</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "      <th>index_st_group</th>\n",
       "      <th>flag_m2_group</th>\n",
       "      <th>DA</th>\n",
       "      <th>stratify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>40382</td>\n",
       "      <td>46886</td>\n",
       "      <td>COSPAN</td>\n",
       "      <td>472D4658</td>\n",
       "      <td>278</td>\n",
       "      <td>489</td>\n",
       "      <td>-78.54106</td>\n",
       "      <td>-7.42856</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2021-10-07-20</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>D02</td>\n",
       "      <td>278--489--2021-10-07-20</td>\n",
       "      <td>1</td>\n",
       "      <td>472D4658-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>33968</td>\n",
       "      <td>24824</td>\n",
       "      <td>PAMPA CANGALLO</td>\n",
       "      <td>4729D600</td>\n",
       "      <td>519</td>\n",
       "      <td>829</td>\n",
       "      <td>-74.19889</td>\n",
       "      <td>-13.56184</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2020-05-03-16</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>D02</td>\n",
       "      <td>519--829--2020-05-03-16</td>\n",
       "      <td>1</td>\n",
       "      <td>4729D600-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.2  Unnamed: 0.1  index  Unnamed: 0          nombre    codigo  \\\n",
       "0             0            29  40382       46886          COSPAN  472D4658   \n",
       "1             1            11  33968       24824  PAMPA CANGALLO  4729D600   \n",
       "\n",
       "    XO   XA  longitud   latitud  ...  umb2          fecha      flag  flagV2  \\\n",
       "0  278  489 -78.54106  -7.42856  ...   8.4  2021-10-07-20  C0000002     D02   \n",
       "1  519  829 -74.19889 -13.56184  ...   4.7  2020-05-03-16  C0000002     D02   \n",
       "\n",
       "                    imagen  clase  index_st_group flag_m2_group DA stratify  \n",
       "0  278--489--2021-10-07-20      1      472D4658-3             0  0        0  \n",
       "1  519--829--2020-05-03-16      1      4729D600-5             0  0        0  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    " Definimos las varibles para las iteraciones\n",
    "\"\"\"\n",
    "\n",
    "modelTipo = 'Clasificacion'\n",
    "rnnTipo = 'LSTM' #'LSTM' CONV3D\n",
    "idProject = datetime.today().strftime(\"%Y%m%d_%H\")\n",
    "\n",
    "p_train = {\n",
    "            # Variables generales\n",
    "          'products': products,\n",
    "          'times'   : times,\n",
    "    \n",
    "            # Reportes\n",
    "          'directory': 'RNN-v2', # 'RNN'\n",
    "          'Proyect'  : f'{modelTipo}-{idProject}', # TesisDiego\n",
    "          'record'   : True,  # Grabar los resultados en  excels    \n",
    "          'WANDB'    : False, # Grabar los resultados en WANDB\n",
    "    \n",
    "            # Datos del modelo\n",
    "          'redTipo'  : modelTipo, # Clasificacion / Regresion\n",
    "          'rnn'      : True,  # Redes recurrentes          \n",
    "          'meanMatrizImagen' : False, # !!! RNN modelos SIEMPRE EN FALSE  !!!\n",
    "          \n",
    "        \n",
    "            # Variables del entrenamiento                \n",
    "          'batch'    : 32,     \n",
    "          'epocas'   : 200,  \n",
    "          'paciencia': 0,   # 0 = No paciencia  (val_accuracy)\n",
    "    \n",
    "    \n",
    "           # Dataset\n",
    "          'dsTrain'  : f'{path_base}/Archivos/Dataset/{modelTipo}/Entrenamiento/VFinal_DA8/CLASE_TrainDS_3.csv',  \n",
    "          'dsVal'    : f'{path_base}/Archivos/Dataset/{modelTipo}/Validacion/ClaseVFinal_M0_ValidacionDS.csv',\n",
    "          'dataset'  : None,     # 1 = 100% del ds\n",
    "          'DA'       : True,  # Usaulmente para clasificacion          \n",
    "         }\n",
    "\n",
    "cantRuns = 1\n",
    "\n",
    " # Hiper parametros     \n",
    "hiperparams = {    \n",
    "               # General\n",
    "              'dsTName'     :['XXXXX_V2']*cantRuns,                         \n",
    "              'inputs'     : [['imagen', 'dato','umb1','altura']]*cantRuns,\n",
    "              'outputs'    : ['clase']*cantRuns,\n",
    "    \n",
    "              # Modelo\n",
    "              'lr'         : [0.001]*cantRuns, \n",
    "              'loss'       : ['binary_crossentropy']*cantRuns,\n",
    "              'normLayer'  : [True]*cantRuns,\n",
    "              'pre_trained': [[]]*cantRuns, #'ResNet50'\n",
    "    \n",
    "               # Capas convulucionales\n",
    "              'cnn_cant'   : [2]*cantRuns,\n",
    "              'cnn_units'  : [[32,64]]*cantRuns,\n",
    "              'droupout'   : [[0.2,0.3]]*cantRuns,\n",
    "              'maxPool'    : [[False,False]]*cantRuns,              \n",
    "    \n",
    "               # Capas Recurrentes\n",
    "              'rnn_tipo'   : [rnnTipo]*cantRuns,\n",
    "              'rnn_units'  : [64]*cantRuns,    \n",
    "              \n",
    "              # Capas densas\n",
    "              'dense_tipo' : ['RELU']*cantRuns, #RELU\n",
    "              'dense_cant' : [2]*cantRuns,\n",
    "              'dense_units': [[128,64]]*cantRuns,\n",
    "              \n",
    "              # Imagenes satelitates\n",
    "              'canales'    : [3]*cantRuns,\n",
    "              'tiempos'    : [6]*cantRuns,\n",
    "              'margen'     : [10]*cantRuns, \n",
    "    \n",
    "              # -\n",
    "              'runs'       : cantRuns\n",
    "              }\n",
    "\n",
    "\n",
    "# Forma del DS\n",
    "if p_train['dsVal']:\n",
    "    dfVal = pd.read_csv(p_train['dsVal'])\n",
    "    print(f'VALDIACION : {len(dfVal)}')\n",
    "    print('--------------------------------------------------')\n",
    "    \n",
    "tempDF = pd.read_csv(p_train['dsTrain'])\n",
    "print(f'TRAIN: {len(tempDF)}')\n",
    "tempDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2db4e92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempDF['DA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64030c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECTORIO BASE : C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/FinalTesis/Tesis2-DiegoParedes/Archivos/Resultados\\Clasificacion\\RNN-v2\\Clasificacion-20230715_10\n",
      "Inicio de la prueba N°: 1/1\n",
      "- Nombre del Proyecto : Clasificacion-20230715_10\n",
      "- Batch size          : 32\n",
      "- Criterio tiempos : 6\n",
      "__________________________________________________\n",
      "Tamaño del dataset: Train 9824  - Val 3101\n",
      "(6, 10, 10, 3)\n",
      "Epoch 1/200\n",
      "307/307 [==============================] - ETA: 0s - loss: 7.0600 - acc: 0.9564 - true_positives: 4784.0000 - true_negatives: 4612.0000 - false_positives: 300.0000 - false_negatives: 128.0000\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/FinalTesis/Tesis2-DiegoParedes/Archivos/Resultados\\Clasificacion\\RNN-v2\\Clasificacion-20230715_10\\Model_01_LSTM_clase_20230715_103041.hdf5\n",
      "307/307 [==============================] - 709s 2s/step - loss: 7.0600 - acc: 0.9564 - true_positives: 4784.0000 - true_negatives: 4612.0000 - false_positives: 300.0000 - false_negatives: 128.0000 - val_loss: 526.0378 - val_acc: 0.0187 - val_true_positives: 0.0000e+00 - val_true_negatives: 58.0000 - val_false_positives: 0.0000e+00 - val_false_negatives: 3043.0000\n",
      "Epoch 2/200\n",
      "122/307 [==========>...................] - ETA: 4:15 - loss: 30.4381 - acc: 0.9019 - true_positives: 3521.0000 - true_negatives: 0.0000e+00 - false_positives: 0.0000e+00 - false_negatives: 383.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultado = trainModel(path_base,path_imagenes,p_train,hiperparams, 'tiempos')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
